--- ISA_SuperApp/__main__.py
+++ ISA_SuperApp/__main__.py
@@ -5,7 +5,6 @@
 when run as a module (python -m isa_superapp).
 """
 
-import asyncio
 import sys
 from pathlib import Path
 

--- src/vc_supply_chain_integration.py
+++ src/vc_supply_chain_integration.py
@@ -6,7 +6,7 @@
 """
 
 import logging
-from typing import Dict, List, Optional, Any, Tuple
+from typing import Dict, List, Optional, Any
 from datetime import datetime, timezone
 
 from .supplier_attestation_vc import (
@@ -16,8 +16,7 @@
     SustainabilityClaim,
     RiskAssessment,
     GeolocationAttestation,
-    ComplianceLevel,
-    AttestationType
+    ComplianceLevel
 )
 from .vc_issuer_service import VCIssuerService, VCVerifierService
 from .geospatial.supply_chain_analysis import SupplyChainGeospatialAnalyzer, SupplyChainNode

--- test_socketio_chat.py
+++ test_socketio_chat.py
@@ -16,7 +16,7 @@
 import logging
 import socketio
 import time
-from typing import Dict, Any, Optional
+from typing import Dict, Any
 
 # Configure logging
 logging.basicConfig(

--- ISA_SuperApp/etl/test_etl.py
+++ ISA_SuperApp/etl/test_etl.py
@@ -6,7 +6,6 @@
 data lineage tracking.
 """
 
-import asyncio
 import sys
 from pathlib import Path
 

--- src/audit_logger.py
+++ src/audit_logger.py
@@ -15,17 +15,15 @@
 import hashlib
 import time
 from datetime import datetime, timedelta
-from typing import Dict, List, Optional, Any, Union
+from typing import Dict, List, Optional, Any
 from enum import Enum
 from dataclasses import dataclass, asdict
-from pathlib import Path
 
 from sqlalchemy import Column, Integer, String, DateTime, Text, Boolean, JSON, create_engine
 from sqlalchemy.ext.declarative import declarative_base
 from sqlalchemy.orm import sessionmaker, Session
 from sqlalchemy.pool import QueuePool
 
-from src.database_manager import get_db_manager
 from src.encryption import EncryptedText
 
 

--- ISA_SuperApp/core/app.py
+++ ISA_SuperApp/core/app.py
@@ -6,18 +6,16 @@
 """
 
 import asyncio
-import logging
 import signal
 import sys
-from pathlib import Path
 from typing import Any, Dict, List, Optional
 
 from .agent_system import AgentOrchestrator
 from .api import APIServer
 from .config import ConfigManager, ISAConfig, get_config
-from .exceptions import ISAConfigurationError, ISAError, ISANotFoundError
+from .exceptions import ISAConfigurationError, ISAError
 from .logger import get_logger, setup_logging
-from .models import Document, SearchQuery, Task, TaskStatus
+from .models import Document, SearchQuery, Task
 from .vector_store import VectorStoreManager
 from .workflow import WorkflowEngine
 

--- gs1_research/GS1_DigitalLink_Resolver_CE/data_entry_server/src/data_entry_db.py
+++ gs1_research/GS1_DigitalLink_Resolver_CE/data_entry_server/src/data_entry_db.py
@@ -1,10 +1,5 @@
-import json
 from pymongo import errors
 from bson import errors as bson_errors
-from bson.objectid import ObjectId
-import os
-import logging
-from pymongo import MongoClient
 from mongo_db_init import mongo
 
 
@@ -97,7 +92,7 @@
 
         # Document ids not found
         if not document_ids:
-            return {"response_status": 404, "error": f"No document ids found"}
+            return {"response_status": 404, "error": "No document ids found"}
 
         return {"response_status": 200, "data": document_ids}
 

--- scripts/deploy_orchestrator.py
+++ scripts/deploy_orchestrator.py
@@ -7,16 +7,13 @@
 """
 
 import yaml
-import json
 import sys
 import time
 import logging
-import requests
 from pathlib import Path
-from typing import Dict, List, Any, Optional, Tuple
-from datetime import datetime, timedelta
+from typing import List, Optional
+from datetime import datetime
 import argparse
-import subprocess
 from dataclasses import dataclass
 from enum import Enum
 

--- ISA_SuperApp/etl/core/vector_store.py
+++ ISA_SuperApp/etl/core/vector_store.py
@@ -5,7 +5,7 @@
 storing processed ETL data.
 """
 
-from typing import Any, Dict, List, Optional
+from typing import Any, Dict, List
 
 from ..core.logger import get_logger
 

--- ISA_SuperApp/cli.py
+++ ISA_SuperApp/cli.py
@@ -11,11 +11,11 @@
 import sys
 from datetime import datetime
 from pathlib import Path
-from typing import Any, Dict, List, Optional
+from typing import Any, Dict, Optional
 
 import click
 
-from .core.app import ISASuperApp, create_app
+from .core.app import create_app
 from .core.config import create_default_config_file, get_config
 from .core.exceptions import ISAError
 from .core.models import Document, SearchQuery, Task, TaskType

--- ISA_SuperApp/__init__.py
+++ ISA_SuperApp/__init__.py
@@ -18,7 +18,7 @@
 __license__ = "MIT"
 __description__ = "ISA SuperApp - Intelligent System Architecture"
 
-logger.debug(f"ISA SuperApp initialization started")
+logger.debug("ISA SuperApp initialization started")
 logger.debug(f"Current working directory: {os.getcwd()}")
 logger.debug(
     f"Available modules in ISA_SuperApp: {os.listdir(os.path.dirname(__file__))}"

--- src/optimized_pipeline.py
+++ src/optimized_pipeline.py
@@ -6,18 +6,17 @@
 import asyncio
 import logging
 import time
-from typing import Dict, List, Optional, Any, Tuple
+from typing import Dict, List, Optional, Any
 from dataclasses import dataclass
 from concurrent.futures import ThreadPoolExecutor
 import threading
-from datetime import datetime, timedelta
+from datetime import datetime
 
 from .database_manager import DatabaseConnectionManager
 from .agent_core.streaming_processor import StreamingDocumentProcessor
 from .cache.multi_level_cache import get_multilevel_cache, MultiLevelCache
 from .docs_provider.optimized_processor import get_optimized_document_processor
 from .semantic_validation import SemanticValidator
-from .docs_provider.optimized_processor import get_optimized_document_processor
 
 logger = logging.getLogger(__name__)
 

--- ISA_SuperApp/core/retrieval.py
+++ ISA_SuperApp/core/retrieval.py
@@ -7,17 +7,14 @@
 
 import abc
 import asyncio
-import json
-import time
 from dataclasses import dataclass
-from typing import Any, Dict, List, Optional, Set, Tuple, Union
+from typing import Any, Dict, List, Optional
 
-import numpy as np
 
 from .embedding import BaseEmbeddingProvider, EmbeddingFactory
-from .exceptions import ISAConfigurationError, ISANotFoundError, ISAValidationError
+from .exceptions import ISAConfigurationError, ISAValidationError
 from .logger import get_logger
-from .models import Document, RetrievalStrategy, SearchResult, Vector
+from .models import Document, SearchResult, Vector
 from .vector_store import BaseVectorStore, VectorStoreConfig, VectorStoreFactory
 
 

--- ISA_SuperApp/version.py
+++ ISA_SuperApp/version.py
@@ -6,7 +6,7 @@
 
 import logging
 import sys
-from typing import Dict, Optional, Tuple
+from typing import Dict, Tuple
 
 __all__ = ["get_version_info", "parse_version", "compare_versions"]
 

--- src/orchestrator/src/orchestrator/agent_core_adapter.py
+++ src/orchestrator/src/orchestrator/agent_core_adapter.py
@@ -43,8 +43,8 @@
         if force_stub or not _AGENT_CORE_AVAILABLE:
             steps = [
                 f"plan: stub for '{goal}'",
-                f"research: stub-web",
-                f"synthesize: stub-report",
+                "research: stub-web",
+                "synthesize: stub-report",
             ]
             return AgentCoreResult(steps=steps, final=f"Final answer: {goal.lower()}")
 

--- gs1_research/GS1_DigitalLink_Resolver_CE/data_entry_server/src/data_entry_logic.py
+++ gs1_research/GS1_DigitalLink_Resolver_CE/data_entry_server/src/data_entry_logic.py
@@ -448,7 +448,7 @@
 
         return result
 
-    except Exception as e:
+    except Exception:
         return {"response_status": 500, "error": "Internal Server Error"}
 
 
@@ -466,7 +466,7 @@
 
         return result
 
-    except Exception as e:
+    except Exception:
         return {"response_status": 500, "error": "Internal Server Error"}
 
 
@@ -481,5 +481,5 @@
         result = data_entry_db.delete_document(anchor)
         return result
 
-    except Exception as e:
+    except Exception:
         return {"response_status": 500, "error": "Internal Server Error"}

--- src/opa_integration.py
+++ src/opa_integration.py
@@ -6,11 +6,9 @@
 and EUDR compliance.
 """
 
-import json
 import logging
-from typing import Dict, List, Any, Optional, Union
+from typing import Dict, List, Any, Optional
 from dataclasses import dataclass
-from pathlib import Path
 
 try:
     import requests
@@ -19,7 +17,6 @@
     REQUESTS_AVAILABLE = False
     logging.warning("requests library not available. Install with: pip install requests")
 
-from .agent_core.llm_client import get_openrouter_free_client
 
 
 @dataclass

--- ISA_SuperApp/etl/assets/eurostat.py
+++ ISA_SuperApp/etl/assets/eurostat.py
@@ -5,19 +5,15 @@
 including economic indicators, population data, and trade statistics.
 """
 
-import json
-import time
-from typing import Any, Dict, List, Optional
+from typing import Any, Dict, List
 
 import pandas as pd
 import requests
 from dagster import (
     AssetExecutionContext,
-    AssetIn,
     MetadataValue,
     Output,
     asset,
-    get_dagster_logger,
 )
 from tenacity import retry, stop_after_attempt, wait_exponential
 

--- scripts/ingest_pdfs.py
+++ scripts/ingest_pdfs.py
@@ -7,13 +7,13 @@
 import sys
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Iterable, List, Optional
+from typing import Iterable, List
 
 
 # Optional deps: PyPDF2 and PyYAML
 try:  # pdf reader
     from PyPDF2 import PdfReader  # type: ignore
-except Exception as e:  # pragma: no cover - optional import
+except Exception:  # pragma: no cover - optional import
     PdfReader = None  # type: ignore
 
 try:  # yaml loader
@@ -63,7 +63,7 @@
         raise RuntimeError("PyPDF2 not installed; run: pip install PyPDF2")
     try:
         reader = PdfReader(str(pdf_path))
-    except Exception as e:  # corrupt or encrypted
+    except Exception:  # corrupt or encrypted
         return []
     pages: List[tuple[int, str]] = []
     limit = min(max_pages, len(reader.pages)) if max_pages > 0 else len(reader.pages)

--- gs1_research/GS1_DigitalLink_Resolver_CE/data_entry_server/src/data_entry_namespace.py
+++ gs1_research/GS1_DigitalLink_Resolver_CE/data_entry_server/src/data_entry_namespace.py
@@ -1,11 +1,9 @@
 import os
 
-from flask import request, jsonify, abort
-from flask_restx import Namespace, Resource, Api, fields
-from functools import wraps
+from flask import request, abort
+from flask_restx import Namespace, Resource, Api
 import logging
 import data_entry_logic
-from werkzeug.exceptions import UnsupportedMediaType
 
 data_entry_namespace = Namespace('', description='Resolver data entry operations')
 

--- ISA_SuperApp/core/logger.py
+++ ISA_SuperApp/core/logger.py
@@ -13,10 +13,9 @@
 from datetime import datetime
 from logging.handlers import RotatingFileHandler
 from pathlib import Path
-from typing import Any, Dict, Optional, Union
+from typing import Any, Dict, Optional
 
 from .config import ISAConfig, LogLevel
-from .exceptions import ISAConfigurationError
 
 
 class ISALogger:

--- ISA_SuperApp/etl/assets/storage.py
+++ ISA_SuperApp/etl/assets/storage.py
@@ -5,14 +5,12 @@
 the database and updating the vector store.
 """
 
-import json
 from typing import Any, Dict, List
 
 import pandas as pd
 from dagster import (
     AssetExecutionContext,
     AssetIn,
-    MetadataValue,
     Output,
     asset,
 )

--- src/orchestrator/tests/test_unified_runner_env_switch.py
+++ src/orchestrator/tests/test_unified_runner_env_switch.py
@@ -1,4 +1,3 @@
-import os
 
 from orchestrator.graph import PlanToolReflect
 

--- ISA_SuperApp/core/workflow.py
+++ ISA_SuperApp/core/workflow.py
@@ -8,22 +8,17 @@
 import abc
 import asyncio
 import enum
-import json
 import time
 import uuid
 from dataclasses import dataclass, field
 from datetime import datetime
-from typing import Any, Callable, Dict, List, Optional, Set, Union
+from typing import Any, Dict, List, Optional
 
-from .agent_system import AgentOrchestrator, BaseAgent, ResearchAgent
-from .exceptions import ISAConfigurationError, ISANotFoundError, ISAValidationError
+from .agent_system import AgentOrchestrator
+from .exceptions import ISANotFoundError, ISAValidationError
 from .logger import get_logger
 from .models import (
     AgentCapability,
-    AgentType,
-    Document,
-    SearchResult,
-    Task,
     TaskPriority,
     TaskStatus,
 )

--- src/agent_core/agent_core/memory/rag_store.py
+++ src/agent_core/agent_core/memory/rag_store.py
@@ -1,10 +1,8 @@
 import hashlib
-import json
 import logging
-import uuid
 from datetime import datetime
 from pathlib import Path
-from typing import Any, Dict, List, Optional, Tuple
+from typing import Any, Dict, List, Optional
 
 import chromadb
 from chromadb.config import Settings

--- src/optimized_agent_workflow.py
+++ src/optimized_agent_workflow.py
@@ -6,7 +6,7 @@
 import asyncio
 import logging
 import time
-from typing import Dict, List, Optional, Any, Tuple
+from typing import Dict, List, Optional, Any
 from dataclasses import dataclass
 from concurrent.futures import ThreadPoolExecutor
 import threading

--- ISA_SuperApp/etl/assets/esma.py
+++ ISA_SuperApp/etl/assets/esma.py
@@ -6,8 +6,7 @@
 """
 
 import json
-import time
-from typing import Any, Dict, List, Optional
+from typing import Any, Dict, List
 
 import pandas as pd
 import requests
@@ -16,7 +15,6 @@
     MetadataValue,
     Output,
     asset,
-    get_dagster_logger,
 )
 from tenacity import retry, stop_after_attempt, wait_exponential
 

--- src/geospatial/risk_assessment.py
+++ src/geospatial/risk_assessment.py
@@ -12,8 +12,7 @@
 
 from shapely.geometry import Point, Polygon
 from .gfw_client import GFWClient, DeforestationAlert, TreeCoverLoss
-from .corine_client import CORINEClient, LandCoverChange
-from .deforestation_scorer import DeforestationRiskScorer
+from .corine_client import CORINEClient
 
 
 @dataclass

--- ISA_SuperApp/core/agent_system.py
+++ ISA_SuperApp/core/agent_system.py
@@ -8,24 +8,19 @@
 import abc
 import asyncio
 import enum
-import json
-import time
 import uuid
 from dataclasses import dataclass, field
 from datetime import datetime
-from typing import Any, Callable, Dict, List, Optional, Set, Union
+from typing import Any, Callable, Dict, List, Optional, Set
 
-from .embedding import BaseEmbeddingProvider, EmbeddingFactory
-from .exceptions import ISAConfigurationError, ISANotFoundError, ISAValidationError
+from .exceptions import ISAConfigurationError, ISAValidationError
 from .logger import get_logger
 from .models import (
-    Agent,
     AgentCapability,
     AgentStatus,
     AgentType,
     Message,
     MessageType,
-    SearchResult,
     Task,
     TaskPriority,
     TaskStatus,

--- src/agent_core/llm_client.py
+++ src/agent_core/llm_client.py
@@ -4,9 +4,7 @@
 import hashlib
 import threading
 from typing import Dict, List, Optional, Any
-import openai
 from openai import OpenAI
-import redis
 from .query_optimizer import QueryOptimizer
 from ..cache.multi_level_cache import get_multilevel_cache, MultiLevelCache
 
@@ -554,7 +552,7 @@
             print(f"‚úÖ Model: {result['model_used']}")
             print(f"üîß Optimization: {'AUTO' if optimization_used else 'MANUAL'}")
             print(f"üìä Tokens: {result['usage']['total_tokens']}")
-            print(f"üí∞ Cost: FREE")
+            print("üí∞ Cost: FREE")
             print(f"üìù Response: {result['content'][:150]}...")
         except Exception as e:
             print(f"‚ùå Error: {e}")

--- scripts/meta_audit.py
+++ scripts/meta_audit.py
@@ -6,9 +6,8 @@
 import os
 import re
 import subprocess
-from dataclasses import dataclass
 from pathlib import Path
-from typing import Iterable, List, Tuple
+from typing import List, Tuple
 
 ROOT = Path(__file__).resolve().parent.parent
 

--- src/api_server.py
+++ src/api_server.py
@@ -10,21 +10,17 @@
 load_dotenv()
 
 from fastapi import FastAPI, Query, Depends, HTTPException, status, Request, Header, Form
-from pydantic import Field
 from fastapi.responses import JSONResponse, PlainTextResponse, Response
 from fastapi.middleware.cors import CORSMiddleware
 from fastapi.middleware.gzip import GZipMiddleware
 from fastapi.middleware.trustedhost import TrustedHostMiddleware
-from fastapi.middleware.httpsredirect import HTTPSRedirectMiddleware
 from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
 from limits import RateLimitItem
 # from limits.aio import RateLimiter
-from limits.storage import MemoryStorage
-from prometheus_client import CONTENT_TYPE_LATEST, Counter, Histogram, generate_latest
+from prometheus_client import Counter, Histogram
 from sqlalchemy.orm import Session
 import json
 import hashlib
-import redis
 from redis import Redis
 from typing import Dict, Any
 import jwt
@@ -40,29 +36,26 @@
 from src.orchestrator.research_graph import ResearchGraph
 from src.tools.web_research import WebResearchTool
 from src.auth import (
-    User, UserCreate, UserUpdate, LoginRequest, Token, TokenData,
-    authenticate_user, create_access_token, verify_token, get_user_by_username,
+    User, UserCreate, UserUpdate, LoginRequest, Token, authenticate_user, create_access_token, verify_token, get_user_by_username,
     check_user_permissions, UserRole, create_user, get_user_by_api_key,
-    get_user_by_email, update_user_password, generate_api_key, hash_api_key,
-    PasswordResetRequest, PasswordResetConfirm, verify_password, JWT_SECRET_KEY, JWT_ALGORITHM,
+    get_user_by_email, update_user_password, PasswordResetRequest, PasswordResetConfirm, verify_password, JWT_SECRET_KEY, JWT_ALGORITHM,
     # OAuth2/OIDC imports
     OAuth2Provider, OAuth2ProviderCreate, OAuth2LoginRequest, OAuth2CallbackRequest, OAuth2TokenResponse,
     get_oauth2_provider_by_name, create_oauth2_provider, oauth2_authorize_url, oauth2_exchange_code,
     oauth2_get_user_info, find_or_create_user_from_oauth2
 )
 from src.audit_logger import (
-    log_auth_event, log_data_access, log_security_event, log_admin_action,
-    AuditEventType, AuditEventSeverity, get_audit_logger
+    log_auth_event, log_data_access, log_admin_action,
+    AuditEventType, get_audit_logger
 )
-from src.database_manager import get_db, get_db_dependency
+from src.database_manager import get_db_dependency
 from infra.monitoring.monitoring_system import monitoring_system, ResearchWorkflowStatus
 # from src.gs1_integration import get_gs1_integration, initialize_gs1_capabilities
-from src.docs_provider.pymupdf_processor import PyMuPDFProcessor, create_pymupdf_processor
-from src.taxonomy.efrag_esrs_loader import EFRAGESRSTaxonomyLoader, create_esrs_loader
+from src.docs_provider.pymupdf_processor import create_pymupdf_processor
+from src.taxonomy.efrag_esrs_loader import create_esrs_loader
 # from src.langgraph_agents.compliance_workflow import ComplianceWorkflowAgent, create_compliance_workflow
 # from src.langgraph_agents.document_analyzer import DocumentAnalyzerAgent, create_document_analyzer
 # from src.langgraph_agents.risk_assessor import RiskAssessorAgent, create_risk_assessor
-from src.database_manager import get_db, get_db_dependency
 
 # Redis cache setup
 redis_client = None
@@ -1166,7 +1159,7 @@
         monitoring_system.record_research_workflow("api_research", ResearchWorkflowStatus.COMPLETED, duration)
 
         return JSONResponse({"query": query, "result_markdown": result_md})
-    except Exception as e:
+    except Exception:
         # Record failure
         duration = time.time() - start_time
         monitoring_system.record_research_workflow("api_research", ResearchWorkflowStatus.FAILED, duration)

--- src/vc_issuer_service.py
+++ src/vc_issuer_service.py
@@ -7,11 +7,10 @@
 
 import json
 import logging
-from typing import Dict, List, Optional, Any, Union
+from typing import Dict, List, Optional, Any
 from datetime import datetime, timezone
 from pathlib import Path
 import base64
-import hashlib
 
 # VC libraries (to be installed)
 try:
@@ -30,8 +29,7 @@
     ComplianceAttestation,
     SustainabilityClaim,
     RiskAssessment,
-    GeolocationAttestation,
-    ComplianceLevel
+    GeolocationAttestation
 )
 
 logger = logging.getLogger(__name__)

--- src/geospatial/corine_client.py
+++ src/geospatial/corine_client.py
@@ -6,17 +6,12 @@
 """
 
 import logging
-import time
 from typing import Dict, List, Optional, Any, Tuple
 from dataclasses import dataclass
-from datetime import datetime
 
 import requests
-import geopandas as gpd
 from shapely.geometry import Point, Polygon
-import pandas as pd
 
-from ..config.performance_config import get_request_timeout
 
 
 @dataclass

--- tests/test_agents.py
+++ tests/test_agents.py
@@ -4,8 +4,7 @@
 
 import asyncio
 from datetime import datetime
-from typing import Any, Dict, List, Optional, Union
-from unittest.mock import AsyncMock, Mock, patch
+from unittest.mock import AsyncMock, Mock
 
 import pytest
 
@@ -19,7 +18,7 @@
     IntegrationAgent,
     ResearchAgent,
 )
-from isa_superapp.core.exceptions import AgentError, ConfigurationError
+from isa_superapp.core.exceptions import AgentError
 
 
 class TestAgentResponse:

--- src/privacy_preserving_ai/isa_integration.py
+++ src/privacy_preserving_ai/isa_integration.py
@@ -6,13 +6,12 @@
 """
 
 import logging
-from typing import Dict, List, Optional, Any, Union
+from typing import Dict, List, Any
 from datetime import datetime
 import asyncio
 
 from .analytics_coordinator import FederatedAnalyticsCoordinator
-from .fhe import FHEContext, ESGDataEncryptor
-from .training_pipeline import TrainingConfig
+from .fhe import FHEContext
 
 # Import ISA_D components (these would be the actual imports in the real system)
 try:

--- ISA_SuperApp/core/config.py
+++ ISA_SuperApp/core/config.py
@@ -7,16 +7,15 @@
 
 import enum
 import json
-import logging
 import os
 from dataclasses import asdict, dataclass, field
 from datetime import datetime
 from pathlib import Path
-from typing import Any, Dict, List, Optional, Type, TypeVar, Union
+from typing import Any, Dict, List, Optional, Type, TypeVar
 
 import yaml
 
-from .exceptions import ISAConfigurationError, ISAValidationError
+from .exceptions import ISAConfigurationError
 from .logger import get_logger
 
 T = TypeVar("T")

--- src/agent_core/agents/planner.py
+++ src/agent_core/agents/planner.py
@@ -4,7 +4,7 @@
 
 from src.docs_provider.base import DocsProvider, NullProvider
 from src.agent_core.llm_client import get_openrouter_free_client
-from src.audit_logger import log_data_access, AuditEventSeverity
+from src.audit_logger import log_data_access
 
 # Configure logging
 logging.basicConfig(

--- scripts/validate_workflow_config.py
+++ scripts/validate_workflow_config.py
@@ -7,10 +7,9 @@
 """
 
 import yaml
-import json
 import sys
 from pathlib import Path
-from typing import Dict, List, Any, Optional
+from typing import Dict, Any
 import argparse
 import logging
 

--- ISA_SuperApp/core/base_agent.py
+++ ISA_SuperApp/core/base_agent.py
@@ -5,14 +5,13 @@
 for all agents in the system.
 """
 
-import asyncio
 import time
 import uuid
 from abc import ABC, abstractmethod
 from dataclasses import dataclass, field
 from datetime import datetime
 from enum import Enum
-from typing import Any, Callable, Dict, List, Optional, Type, TypeVar, Union
+from typing import Any, Callable, Dict, List, Optional, TypeVar
 
 from .config import ISAConfig
 from .exceptions import ISAConfigurationError, ISAValidationError
@@ -137,7 +136,7 @@
         self._event_handlers: Dict[str, List[Callable]] = {}
 
         self.logger.info(
-            f"Agent initialized",
+            "Agent initialized",
             agent_id=self.agent_id,
             name=self.name,
             capabilities=[cap.value for cap in self.capabilities],

--- src/geospatial/gfw_client.py
+++ src/geospatial/gfw_client.py
@@ -12,9 +12,7 @@
 from datetime import datetime, timedelta
 
 import requests
-import geopandas as gpd
 from shapely.geometry import Point, Polygon
-import pandas as pd
 
 from ..config.performance_config import get_request_timeout
 

--- src/encryption.py
+++ src/encryption.py
@@ -11,12 +11,11 @@
 
 import os
 import base64
-from typing import Optional, Any
+from typing import Optional
 from cryptography.fernet import Fernet
 from cryptography.hazmat.primitives import hashes
 from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
 from sqlalchemy import Column, String, Text
-from sqlalchemy.ext.declarative import declared_attr
 from sqlalchemy.orm import column_property
 
 

--- tests/test_core_app.py
+++ tests/test_core_app.py
@@ -3,10 +3,9 @@
 """
 
 import asyncio
-import json
 import tempfile
 from pathlib import Path
-from unittest.mock import AsyncMock, Mock, patch
+from unittest.mock import patch
 
 import pytest
 

--- src/privacy_preserving_ai/analytics_coordinator.py
+++ src/privacy_preserving_ai/analytics_coordinator.py
@@ -8,15 +8,14 @@
 
 import asyncio
 import logging
-from typing import Dict, List, Optional, Any, Union
+from typing import Dict, List, Optional, Any
 from dataclasses import dataclass
 from datetime import datetime
-import json
 
 from .fhe import FHEContext, ESGDataEncryptor
-from .fl_server import create_fl_server, FederatedLearningServer
+from .fl_server import create_fl_server
 from .fl_client import create_fl_client, FederatedLearningClient
-from .training_pipeline import EncryptedTrainingPipeline, TrainingConfig, ESGModel
+from .training_pipeline import EncryptedTrainingPipeline, TrainingConfig
 
 logger = logging.getLogger(__name__)
 

--- ISA_SuperApp/core/rerank.py
+++ ISA_SuperApp/core/rerank.py
@@ -6,11 +6,9 @@
 """
 
 import abc
-import asyncio
 from dataclasses import dataclass
-from typing import Any, Dict, List, Optional, Tuple
+from typing import List, Optional
 
-import numpy as np
 
 from .exceptions import ISAConfigurationError, ISAValidationError
 from .logger import get_logger

--- src/agent_core/query_optimizer.py
+++ src/agent_core/query_optimizer.py
@@ -1,5 +1,5 @@
 import re
-from typing import Dict, List, Optional, Any, Tuple
+from typing import Dict, Any
 from dataclasses import dataclass
 from enum import Enum
 

--- src/end_to_end_traceability.py
+++ src/end_to_end_traceability.py
@@ -8,12 +8,11 @@
 import json
 import hashlib
 from datetime import datetime, timezone
-from typing import Dict, List, Optional, Any, Union
+from typing import Dict, List, Optional, Any
 from dataclasses import dataclass, asdict
-from pathlib import Path
 import logging
 
-from .epcis_tracker import EPCISEvent, EPCISDocument
+from .epcis_tracker import EPCISEvent
 from .webvoc_loader import GS1WebVocLoader
 
 logger = logging.getLogger(__name__)

--- compliance_policies/test_opa_integration.py
+++ compliance_policies/test_opa_integration.py
@@ -7,8 +7,6 @@
 """
 
 import sys
-import os
-import json
 from pathlib import Path
 
 # Add the src directory to the path so we can import our modules

--- scripts/perf_budget_check.py
+++ scripts/perf_budget_check.py
@@ -3,7 +3,6 @@
 
 import json
 import os
-import sys
 from pathlib import Path
 
 

--- src/agent_core/tests/test_query_optimizer.py
+++ src/agent_core/tests/test_query_optimizer.py
@@ -1,5 +1,4 @@
 import pytest
-from unittest.mock import Mock, patch
 from src.agent_core.query_optimizer import (
     QueryOptimizer, QueryAnalysis, OptimizationResult,
     QueryDomain, QueryComplexity

--- src/privacy_preserving_ai/training_pipeline.py
+++ src/privacy_preserving_ai/training_pipeline.py
@@ -6,14 +6,11 @@
 """
 
 import logging
-from typing import Dict, List, Optional, Any, Union
+from typing import Dict, List, Optional, Any
 from dataclasses import dataclass
 from datetime import datetime
-import numpy as np
 
 from .fhe import FHEContext, ESGDataEncryptor, FHEOperations
-from .fl_server import create_fl_server, FederatedLearningServer
-from .fl_client import create_fl_client, FederatedLearningClient
 
 logger = logging.getLogger(__name__)
 

--- tests/test_vector_store.py
+++ tests/test_vector_store.py
@@ -3,7 +3,7 @@
 """
 
 from datetime import datetime
-from typing import Any, Dict, List, Optional, Union
+from typing import Any, Dict, List, Optional
 from unittest.mock import AsyncMock, Mock, patch
 
 import numpy as np

--- ISA_SuperApp/core/vector_store.py
+++ ISA_SuperApp/core/vector_store.py
@@ -7,16 +7,14 @@
 
 import abc
 import asyncio
-import json
-import time
 from dataclasses import dataclass
-from typing import Any, Dict, List, Optional, Set, Tuple, Union
+from typing import Any, Dict, List, Optional
 
 import numpy as np
 
-from .exceptions import ISAConfigurationError, ISANotFoundError, ISAValidationError
+from .exceptions import ISAConfigurationError, ISAValidationError
 from .logger import get_logger
-from .models import SearchResult, Vector, VectorStoreProvider
+from .models import SearchResult, Vector
 
 
 @dataclass

--- infra/monitoring/monitoring_system.py
+++ infra/monitoring/monitoring_system.py
@@ -4,19 +4,18 @@
 business KPIs, system health, and ISA-specific metrics.
 """
 
-import time
 import logging
 import psutil
-from typing import Dict, Any, Optional, List
+from typing import Optional
 from dataclasses import dataclass
 from enum import Enum
 from prometheus_client import (
-    Counter, Histogram, Gauge, Summary, CollectorRegistry,
+    Counter, Histogram, Gauge, CollectorRegistry,
     generate_latest, CONTENT_TYPE_LATEST
 )
 from fastapi import Response
 import asyncio
-from datetime import datetime, timedelta
+from datetime import datetime
 
 logger = logging.getLogger(__name__)
 

--- src/auth.py
+++ src/auth.py
@@ -13,24 +13,23 @@
 import secrets
 import string
 from datetime import datetime, timedelta
-from typing import Optional, List, Dict, Any
+from typing import Optional, Dict, Any
 from enum import Enum
 
 import bcrypt
 import jwt
 from pydantic import BaseModel, EmailStr, Field
-from sqlalchemy import Column, Integer, String, DateTime, Boolean, Text, create_engine
+from sqlalchemy import Column, Integer, String, DateTime, Boolean
 from sqlalchemy.ext.declarative import declarative_base
-from sqlalchemy.orm import sessionmaker, Session
-from src.database_manager import get_db_manager, get_db
+from sqlalchemy.orm import Session
+from src.database_manager import get_db_manager
 
 # OAuth2/OIDC imports
 from authlib.integrations.base_client import OAuthError
 from authlib.integrations.httpx_client import AsyncOAuth2Client
-import httpx
 
 # Encryption imports
-from src.encryption import EncryptedText, encrypt_sensitive_data, decrypt_sensitive_data
+from src.encryption import EncryptedText
 
 # Database setup with connection pooling
 DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./isa_auth.db")

--- scripts/inspect_vector_store.py
+++ scripts/inspect_vector_store.py
@@ -95,7 +95,7 @@
         
         # Get basic stats
         stats = rag_memory.get_stats()
-        print(f"üìä Vector Store Statistics:")
+        print("üìä Vector Store Statistics:")
         print(f"   Total chunks: {stats['total_chunks']}")
         print(f"   Embedding model: {stats['embedding_model']}")
         print(f"   Persist directory: {stats['persist_directory']}")

--- src/geospatial/supply_chain_analysis.py
+++ src/geospatial/supply_chain_analysis.py
@@ -7,16 +7,11 @@
 """
 
 import logging
-from typing import Dict, List, Any, Optional, Tuple, Set
+from typing import Dict, List, Any, Optional, Tuple
 from dataclasses import dataclass
-from datetime import datetime
 import networkx as nx
 
-from shapely.geometry import Point, Polygon, LineString
-from shapely.ops import nearest_points
 from .risk_assessment import GeospatialRiskAssessor, LocationRiskAssessment
-from .gfw_client import DeforestationAlert
-from .corine_client import LandCoverChange
 
 
 @dataclass

--- src/privacy_preserving_ai/fl_client.py
+++ src/privacy_preserving_ai/fl_client.py
@@ -8,9 +8,7 @@
 import asyncio
 import logging
 from typing import Dict, List, Optional, Any, Union
-import numpy as np
 import json
-from datetime import datetime
 
 try:
     import aiohttp

--- scripts/tests/test_ingest_text.py
+++ scripts/tests/test_ingest_text.py
@@ -3,7 +3,6 @@
 import hashlib
 import sys
 from importlib import import_module
-from pathlib import Path
 
 
 def test_ingest_text_writes_manifest(tmp_path, monkeypatch):

--- tests/test_main.py
+++ tests/test_main.py
@@ -2,20 +2,15 @@
 Tests for main application components.
 """
 
-import asyncio
-import json
-from datetime import datetime
-from typing import Any, Dict, List, Optional
 from unittest.mock import AsyncMock, Mock, patch
 
 import pytest
 
 from isa_superapp.core.config import Config
-from isa_superapp.core.exceptions import ConfigurationError, ISAError
+from isa_superapp.core.exceptions import ConfigurationError
 from isa_superapp.main import ISASuperApp, create_app, main
 from isa_superapp.orchestrator.base import TaskDefinition, TaskPriority, TaskStatus
 from isa_superapp.retrieval.base import Document, SearchResult
-from isa_superapp.vector_store.base import VectorDocument
 
 
 class TestISASuperApp:

--- src/agent_core/tests/test_phase2_integration.py
+++ src/agent_core/tests/test_phase2_integration.py
@@ -2,11 +2,11 @@
 import time
 import threading
 import statistics
-from unittest.mock import Mock, patch, MagicMock
+from unittest.mock import patch
 from pathlib import Path
 
 # Import all Phase 2 components
-from src.agent_core.llm_client import CachedOpenRouterClient, ModelWarmer
+from src.agent_core.llm_client import CachedOpenRouterClient
 from src.database_manager import DatabaseConnectionManager
 from src.agent_core.streaming_processor import StreamingDocumentProcessor
 

--- src/semantic_validation/engine.py
+++ src/semantic_validation/engine.py
@@ -9,7 +9,7 @@
 from typing import Dict, List, Any, Optional, Tuple
 from dataclasses import dataclass
 
-from rdflib import Graph, URIRef, Literal
+from rdflib import Graph
 from pyshacl import validate as pyshacl_validate
 
 logger = logging.getLogger(__name__)

--- ISA_SuperApp/core/embedding.py
+++ ISA_SuperApp/core/embedding.py
@@ -7,10 +7,8 @@
 
 import abc
 import asyncio
-import json
-import time
 from dataclasses import dataclass
-from typing import Any, Dict, List, Optional, Tuple, Type, Union
+from typing import Any, Dict, List, Optional, Type
 
 import numpy as np
 

--- scripts/r2p_check.py
+++ scripts/r2p_check.py
@@ -2,9 +2,7 @@
 from __future__ import annotations
 
 import os
-import re
 import subprocess
-import sys
 from pathlib import Path
 
 

--- src/privacy_preserving_ai/fl_server.py
+++ src/privacy_preserving_ai/fl_server.py
@@ -5,12 +5,10 @@
 model updates from multiple clients without accessing their raw data.
 """
 
-import asyncio
 import logging
 from typing import Dict, List, Optional, Any, Set
 from dataclasses import dataclass
 from datetime import datetime
-import json
 
 try:
     import aiohttp

--- tests/test_vc_system.py
+++ tests/test_vc_system.py
@@ -2,9 +2,8 @@
 Test suite for the VC system implementation.
 """
 
-import pytest
 import json
-from datetime import datetime, timezone, timedelta
+from datetime import datetime, timezone
 from pathlib import Path
 import tempfile
 
@@ -12,14 +11,10 @@
     SupplierAttestationVCManager,
     SupplierInfo,
     ComplianceAttestation,
-    SustainabilityClaim,
-    RiskAssessment,
-    GeolocationAttestation,
-    ComplianceLevel,
-    AttestationType
+    ComplianceLevel
 )
 from src.vc_issuer_service import VCIssuerService, VCVerifierService
-from src.vc_storage import VCStorage, VCRegistry
+from src.vc_storage import VCStorage
 from src.vc_supply_chain_integration import VCSupplyChainIntegrator
 
 class TestVCSystem:

--- src/geospatial/deforestation_scorer.py
+++ src/geospatial/deforestation_scorer.py
@@ -8,7 +8,7 @@
 
 import logging
 from typing import Dict, List, Any, Optional, Tuple
-from datetime import datetime, timedelta
+from datetime import datetime
 from dataclasses import dataclass
 import math
 

--- src/semantic_validation/schemas/esg_schemas.py
+++ src/semantic_validation/schemas/esg_schemas.py
@@ -6,10 +6,9 @@
 """
 
 import logging
-from typing import Dict, Any, Optional
 
 from rdflib import Graph, URIRef, Literal, BNode, Namespace
-from rdflib.namespace import RDF, RDFS, XSD
+from rdflib.namespace import RDF, XSD
 
 logger = logging.getLogger(__name__)
 

--- scripts/test_security.py
+++ scripts/test_security.py
@@ -15,24 +15,23 @@
 import sys
 import asyncio
 import json
-from datetime import datetime, timedelta
+from datetime import datetime
 from pathlib import Path
 
 # Add src to path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
 
 from src.auth import (
-    User, UserCreate, UserRole, create_user, authenticate_user,
+    UserCreate, UserRole, create_user, authenticate_user,
     get_oauth2_provider_by_name, create_oauth2_provider, OAuth2ProviderCreate,
     get_db_manager
 )
 from src.encryption import (
-    get_encryption_service, encrypt_sensitive_data, decrypt_sensitive_data,
-    generate_encryption_key, validate_encryption_key
+    get_encryption_service, generate_encryption_key, validate_encryption_key
 )
 from src.audit_logger import (
     get_audit_logger, AuditEventType, AuditEventSeverity,
-    log_auth_event, log_data_access, log_security_event
+    log_auth_event, log_data_access
 )
 
 

--- src/vc_storage.py
+++ src/vc_storage.py
@@ -7,11 +7,10 @@
 
 import json
 import logging
-from typing import Dict, List, Optional, Any, Set
+from typing import Dict, List, Optional, Any
 from datetime import datetime, timezone
 from pathlib import Path
 import sqlite3
-from dataclasses import asdict
 
 from .supplier_attestation_vc import SupplierAttestationCredential
 

--- src/agent_core/tests/test_performance_benchmarks.py
+++ src/agent_core/tests/test_performance_benchmarks.py
@@ -3,7 +3,7 @@
 import statistics
 import threading
 from pathlib import Path
-from unittest.mock import Mock, patch
+from unittest.mock import Mock
 from src.agent_core.query_optimizer import QueryOptimizer
 from src.agent_core.streaming_processor import StreamingDocumentProcessor
 from src.agent_core.memory.rag_store import RAGMemory
@@ -386,7 +386,7 @@
         total_workflow_time = workflow_end - workflow_start
 
         # Performance analysis
-        print(f"End-to-end workflow performance:")
+        print("End-to-end workflow performance:")
         print(f"Total time: {total_workflow_time:.4f}s")
         print(f"Query optimization: {sum(o['optimization_time'] for o in optimizations):.4f}s")
         print(f"Document processing: {sum(p['processing_time'] for p in processing_results):.4f}s")

--- test_taxonomy_simple.py
+++ test_taxonomy_simple.py
@@ -9,13 +9,12 @@
 import os
 import json
 import tempfile
-from pathlib import Path
 from datetime import datetime
 
 # Add src to path
 sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))
 
-from taxonomy.efrag_esrs_loader import EFRAGESRSTaxonomyLoader, ESRSTaxonomy, TaxonomyElement, TaxonomyTable
+from taxonomy.efrag_esrs_loader import EFRAGESRSTaxonomyLoader
 from taxonomy.models import create_taxonomy_tables
 from database_manager import get_db_manager
 

--- tests/test_geospatial_screening.py
+++ tests/test_geospatial_screening.py
@@ -8,7 +8,7 @@
 import pytest
 import unittest.mock as mock
 from datetime import datetime, timedelta
-from shapely.geometry import Point, Polygon
+from shapely.geometry import Point
 
 from src.geospatial.gfw_client import GFWClient, DeforestationAlert, TreeCoverLoss
 from src.geospatial.corine_client import CORINEClient

--- scripts/smoke_tests.py
+++ scripts/smoke_tests.py
@@ -7,7 +7,6 @@
 import requests
 import sys
 import time
-from typing import Optional
 
 
 def test_health_endpoint(endpoint: str, timeout: int = 30) -> bool:
@@ -57,7 +56,7 @@
     try:
         response = requests.get(f"{endpoint}/api/v1/health/db", timeout=timeout)
         if response.status_code == 200:
-            print(f"‚úÖ Database connection test passed")
+            print("‚úÖ Database connection test passed")
             return True
         else:
             print(f"‚ùå Database connection test failed: {response.status_code}")

--- src/semantic_validation/schemas/regulatory_schemas.py
+++ src/semantic_validation/schemas/regulatory_schemas.py
@@ -6,10 +6,9 @@
 """
 
 import logging
-from typing import Dict, Any, Optional
 
 from rdflib import Graph, URIRef, Literal, BNode, Namespace
-from rdflib.namespace import RDF, RDFS, XSD
+from rdflib.namespace import RDF, XSD
 
 logger = logging.getLogger(__name__)
 

--- src/geospatial/screening_engine.py
+++ src/geospatial/screening_engine.py
@@ -13,7 +13,7 @@
 
 from .gfw_client import GFWClient
 from .corine_client import CORINEClient
-from .risk_assessment import GeospatialRiskAssessor, LocationRiskAssessment, SupplyChainRiskProfile
+from .risk_assessment import GeospatialRiskAssessor, LocationRiskAssessment
 from .supply_chain_analysis import SupplyChainGeospatialAnalyzer, SupplyChainNode
 from .deforestation_scorer import DeforestationRiskScorer
 

--- tests/test_api_server.py
+++ tests/test_api_server.py
@@ -3,10 +3,9 @@
 """
 
 import pytest
-from unittest.mock import Mock, patch, AsyncMock
+from unittest.mock import Mock, patch
 from fastapi.testclient import TestClient
 from fastapi import HTTPException
-import json
 
 from src.api_server import app
 from src.auth import User, UserRole, create_access_token

--- src/neo4j_gds_schema.py
+++ src/neo4j_gds_schema.py
@@ -161,7 +161,7 @@
    - GraphSAGE: Inductive learning for new nodes
 """
 
-from typing import Dict, List, Any, Optional
+from typing import Dict, List, Any
 from dataclasses import dataclass
 from datetime import datetime
 from enum import Enum

--- src/agent_core/tests/test_integration_isa.py
+++ src/agent_core/tests/test_integration_isa.py
@@ -1,11 +1,10 @@
 import pytest
 import time
 from pathlib import Path
-from unittest.mock import Mock, patch
+from unittest.mock import Mock
 from src.agent_core.query_optimizer import QueryOptimizer
 from src.agent_core.streaming_processor import StreamingDocumentProcessor
 from src.agent_core.memory.rag_store import RAGMemory
-from src.agent_core.llm_client import CachedOpenRouterClient
 
 
 class TestISAIntegration:

--- src/semantic_validation/schemas/gs1_schemas.py
+++ src/semantic_validation/schemas/gs1_schemas.py
@@ -6,10 +6,9 @@
 """
 
 import logging
-from typing import Dict, Any, Optional
 
 from rdflib import Graph, URIRef, Literal, BNode, Namespace
-from rdflib.namespace import RDF, RDFS, XSD
+from rdflib.namespace import RDF, XSD
 
 logger = logging.getLogger(__name__)
 

--- test_openrouter_free_integration.py
+++ test_openrouter_free_integration.py
@@ -59,14 +59,14 @@
 
             print(f"‚úÖ Model Used: {result['model_used']}")
             print(f"üìä Tokens: {result['usage']['total_tokens']}")
-            print(f"üí∞ Cost: FREE")
+            print("üí∞ Cost: FREE")
             print(f"üìù Response Preview: {result['content'][:150]}...")
 
             # Verify model selection logic
             if test_case['expected_model'] in result['model_used'].lower():
                 print(f"‚úÖ Correct model selected for {test_case['expected_model']} scenario")
             else:
-                print(f"‚ö†Ô∏è  Model selection may need optimization")
+                print("‚ö†Ô∏è  Model selection may need optimization")
 
         except Exception as e:
             print(f"‚ùå Error: {e}")

--- tests/utils.py
+++ tests/utils.py
@@ -15,11 +15,10 @@
 import time
 from contextlib import contextmanager
 from pathlib import Path
-from typing import Any, Callable, Dict, List, Optional, Union
+from typing import Any, Callable, Dict, List, Optional
 from unittest.mock import AsyncMock, MagicMock, patch
 
 import numpy as np
-import pytest
 from faker import Faker
 
 # Initialize Faker for generating test data

--- tests/test_auth.py
+++ tests/test_auth.py
@@ -4,7 +4,6 @@
 
 import pytest
 from unittest.mock import Mock, patch
-from datetime import datetime, timedelta
 from sqlalchemy.orm import Session
 
 from src.auth import (

--- src/agent_core/tests/test_error_handling.py
+++ src/agent_core/tests/test_error_handling.py
@@ -1,7 +1,6 @@
 import pytest
-import json
 from pathlib import Path
-from unittest.mock import Mock, patch, MagicMock
+from unittest.mock import Mock, patch
 from src.agent_core.query_optimizer import QueryOptimizer
 from src.agent_core.streaming_processor import StreamingDocumentProcessor
 from src.agent_core.memory.rag_store import RAGMemory

--- src/benchmark_suite.py
+++ src/benchmark_suite.py
@@ -3,11 +3,10 @@
 Advanced benchmarking framework with automated performance testing and optimization validation.
 """
 
-import asyncio
 import logging
 import time
 import statistics
-from typing import Dict, List, Optional, Any, Tuple
+from typing import Dict, List, Optional, Any
 from dataclasses import dataclass
 from datetime import datetime
 from pathlib import Path

--- src/semantic_validation/converter.py
+++ src/semantic_validation/converter.py
@@ -6,7 +6,7 @@
 """
 
 import logging
-from typing import Dict, List, Any, Optional, Union
+from typing import Dict, Any, Optional
 from datetime import datetime
 
 from rdflib import Graph, URIRef, Literal, BNode, Namespace

--- tests/test_retrieval.py
+++ tests/test_retrieval.py
@@ -3,10 +3,8 @@
 """
 
 from datetime import datetime
-from typing import Any, Dict, List, Optional, Union
 from unittest.mock import AsyncMock, Mock, patch
 
-import numpy as np
 import pytest
 
 from isa_superapp.core.exceptions import ConfigurationError, RetrievalError

--- scripts/research/tests/test_research_live.py
+++ scripts/research/tests/test_research_live.py
@@ -1,7 +1,5 @@
 import os
-import pytest
 from unittest.mock import Mock, patch
-from pathlib import Path
 
 from scripts.research.fetcher import fetch, _is_domain_allowed, _rate_limit_domain
 

--- src/agent_core/tests/test_llm_caching.py
+++ src/agent_core/tests/test_llm_caching.py
@@ -1,8 +1,7 @@
 import pytest
 import json
-import time
-from unittest.mock import Mock, patch, MagicMock
-from src.agent_core.llm_client import CachedOpenRouterClient, OpenRouterFreeClient
+from unittest.mock import Mock, patch
+from src.agent_core.llm_client import CachedOpenRouterClient
 
 
 class TestCachedOpenRouterClient:

--- tests/test_llm_providers.py
+++ tests/test_llm_providers.py
@@ -2,9 +2,7 @@
 Tests for LLM provider functionality.
 """
 
-import json
-from typing import Any, Dict, List
-from unittest.mock import AsyncMock, Mock, patch
+from unittest.mock import Mock, patch
 
 import pytest
 

--- src/performance_monitor.py
+++ src/performance_monitor.py
@@ -3,13 +3,12 @@
 Comprehensive performance monitoring with intelligent alerting and optimization recommendations.
 """
 
-import asyncio
 import logging
 import time
 import threading
-from typing import Dict, List, Optional, Any, Callable
+from typing import Dict, List, Optional, Any
 from dataclasses import dataclass
-from datetime import datetime, timedelta
+from datetime import datetime
 from enum import Enum
 import json
 import statistics

--- gs1_research/GS1_DigitalLink_Resolver_CE/web_server/src/web_logic.py
+++ gs1_research/GS1_DigitalLink_Resolver_CE/web_server/src/web_logic.py
@@ -644,7 +644,7 @@
     except Exception as e:
         print(f"get_compressed_link - Unexpected error occurred. Details: {str(e)}")
         return {'response_status': 400,
-                'error': f"Unexpected error occurred. Check GS1 Digital Link syntax is correct before compressing"}
+                'error': "Unexpected error occurred. Check GS1 Digital Link syntax is correct before compressing"}
 
 
 def _clean_q_values_from_header_entries(header_values_list):

--- src/semantic_validation/validator.py
+++ src/semantic_validation/validator.py
@@ -6,13 +6,11 @@
 """
 
 import logging
-from typing import Dict, List, Any, Optional, Union
+from typing import Dict, List, Any, Optional
 from dataclasses import dataclass, field
 from datetime import datetime
 
-from rdflib import Graph, URIRef, Literal, BNode
-from rdflib.namespace import RDF, RDFS, XSD
-from pyshacl import validate
+from rdflib import Graph
 
 from .engine import SHACLEngine
 from .converter import RDFConverter

--- src/neo4j_gds_ingestion.py
+++ src/neo4j_gds_ingestion.py
@@ -14,17 +14,14 @@
 """
 
 import logging
-import time
-from typing import Dict, List, Any, Optional, Union, Iterator
+from typing import Dict, List, Any, Optional
 from datetime import datetime, timezone
 from dataclasses import dataclass
 from concurrent.futures import ThreadPoolExecutor, as_completed
-from queue import Queue
-import json
 
 from .neo4j_gds_client import Neo4jGDSClient, get_gds_client
-from .neo4j_gds_schema import SupplyChainGraphSchema, NodeType, RelationshipType
-from .epcis_tracker import EPCISEvent, EPCISDocument, EventType, Action, BizStep
+from .neo4j_gds_schema import SupplyChainGraphSchema, RelationshipType
+from .epcis_tracker import EPCISEvent, EventType
 from .gs1_integration import GS1IntegrationManager
 
 logger = logging.getLogger(__name__)

--- tests/conftest.py
+++ tests/conftest.py
@@ -7,16 +7,14 @@
 """
 
 import asyncio
-import json
 import logging
 import os
 import tempfile
 from pathlib import Path
-from typing import Any, AsyncGenerator, Dict, Generator, List, Optional
-from unittest.mock import AsyncMock, MagicMock, patch
+from typing import Any, AsyncGenerator, Dict, Generator, List
+from unittest.mock import MagicMock
 
 import pytest
-import pytest_asyncio
 from _pytest.config import Config
 from _pytest.monkeypatch import MonkeyPatch
 
@@ -154,7 +152,6 @@
     """Provide a test database session."""
     # This would typically set up a test database
     # For now, we'll use an in-memory SQLite database
-    from sqlalchemy import create_engine
     from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
     from sqlalchemy.orm import sessionmaker
 
@@ -188,7 +185,6 @@
 @pytest.fixture
 async def test_client() -> AsyncGenerator[Any, None]:
     """Provide a test client for the FastAPI application."""
-    from fastapi.testclient import TestClient
 
     # Import your FastAPI app here
     # from isa_superapp.api.app import app
@@ -396,7 +392,6 @@
 @pytest.fixture(scope="session")
 def performance_monitor() -> Generator[Any, None, None]:
     """Monitor performance during test execution."""
-    import os
     import time
 
     import psutil

--- src/agent_core/tests/test_model_prewarming.py
+++ src/agent_core/tests/test_model_prewarming.py
@@ -1,7 +1,6 @@
 import pytest
 import time
-import threading
-from unittest.mock import Mock, patch, MagicMock
+from unittest.mock import Mock, patch
 from src.agent_core.llm_client import ModelWarmer, CachedOpenRouterClient
 
 

--- scripts/import_guard.py
+++ scripts/import_guard.py
@@ -2,7 +2,6 @@
 from __future__ import annotations
 
 import re
-import sys
 from pathlib import Path
 
 

--- src/docs_provider/src/docs_provider/cache.py
+++ src/docs_provider/src/docs_provider/cache.py
@@ -1,6 +1,4 @@
 import hashlib
-import json
-import time
 from pathlib import Path
 from typing import Any, Optional
 import sys

--- tests/unit/test_vector_store.py
+++ tests/unit/test_vector_store.py
@@ -6,12 +6,11 @@
 including ChromaDB integration, embedding management, and vector operations.
 """
 
-from typing import Any, Dict, List, Optional
-from unittest.mock import AsyncMock, MagicMock, patch
+from unittest.mock import MagicMock, patch
 
 import pytest
 
-from tests.utils import MockFactory, TestDataGenerator, TestValidators
+from tests.utils import TestDataGenerator
 
 
 class TestChromaDBIntegration:

--- src/neo4j_gds_client.py
+++ src/neo4j_gds_client.py
@@ -14,13 +14,13 @@
 
 import logging
 import time
-from typing import Dict, List, Any, Optional, Union
+from typing import Dict, List, Any, Optional
 from contextlib import contextmanager
 from dataclasses import dataclass
-from datetime import datetime, timedelta
+from datetime import datetime
 
 from neo4j import GraphDatabase, Driver
-from neo4j.exceptions import ServiceUnavailable, AuthError, DatabaseError
+from neo4j.exceptions import ServiceUnavailable, AuthError
 from graphdatascience import GraphDataScience
 import pandas as pd
 

--- docs/conf.py
+++ docs/conf.py
@@ -1,5 +1,3 @@
-import os
-import sys
 from datetime import datetime
 
 project = "ISA SuperApp"

--- tests/test_vector_stores.py
+++ tests/test_vector_stores.py
@@ -2,13 +2,12 @@
 Tests for vector store functionality.
 """
 
-from typing import Any, Dict, List, Optional
-from unittest.mock import AsyncMock, Mock, patch
+from unittest.mock import Mock, patch
 
 import numpy as np
 import pytest
 
-from isa_superapp.core.exceptions import VectorStoreConnectionError, VectorStoreError
+from isa_superapp.core.exceptions import VectorStoreError
 from isa_superapp.vector_stores.base import SearchResult, VectorStore
 from isa_superapp.vector_stores.chroma_store import ChromaVectorStore
 from isa_superapp.vector_stores.faiss_store import FaissVectorStore

--- tests/unit/test_memory.py
+++ tests/unit/test_memory.py
@@ -7,12 +7,11 @@
 """
 
 from datetime import datetime, timedelta
-from typing import Any, Dict, List, Optional
-from unittest.mock import AsyncMock, MagicMock, patch
+from unittest.mock import MagicMock
 
 import pytest
 
-from tests.utils import MockFactory, TestDataGenerator, TestValidators
+from tests.utils import TestDataGenerator
 
 
 class TestConversationMemory:

--- src/docs_provider/optimized_processor.py
+++ src/docs_provider/optimized_processor.py
@@ -4,7 +4,6 @@
 """
 
 import asyncio
-import concurrent.futures
 import hashlib
 import logging
 import time

--- src/agent_core/tests/test_streaming_processor.py
+++ src/agent_core/tests/test_streaming_processor.py
@@ -1,6 +1,6 @@
 import pytest
 import time
-from unittest.mock import Mock, patch, AsyncMock
+from unittest.mock import patch
 from src.agent_core.streaming_processor import StreamingDocumentProcessor, Chunk, ProcessingResult
 
 

--- src/dmn/test_dmn_compliance.py
+++ src/dmn/test_dmn_compliance.py
@@ -4,10 +4,8 @@
 This module provides test cases and examples for the DMN compliance automation system.
 """
 
-import json
 import logging
 from pathlib import Path
-from typing import Dict, Any
 
 from .dmn_manager import DMNManager
 from .dmn_compliance_integration import DMNComplianceIntegration

--- src/agent_core/tests/test_rag_memory_enhanced.py
+++ src/agent_core/tests/test_rag_memory_enhanced.py
@@ -2,7 +2,6 @@
 import time
 import threading
 from pathlib import Path
-from unittest.mock import Mock, patch
 from src.agent_core.memory.rag_store import RAGMemory, SearchFilters
 
 

--- tests/unit/test_llm.py
+++ tests/unit/test_llm.py
@@ -6,12 +6,10 @@
 including model initialization, text generation, and prompt management.
 """
 
-from typing import Any, Dict, List, Optional
-from unittest.mock import AsyncMock, MagicMock, patch
+from unittest.mock import MagicMock, patch
 
 import pytest
 
-from tests.utils import MockFactory, TestDataGenerator, TestValidators
 
 
 class TestLLMInitialization:

--- src/docs_provider/tests/test_pymupdf_processor.py
+++ src/docs_provider/tests/test_pymupdf_processor.py
@@ -7,7 +7,6 @@
 from unittest.mock import patch, MagicMock
 from src.docs_provider.pymupdf_processor import (
     PyMuPDFProcessor,
-    PDFProcessingResult,
     PDFMetadata,
     PYMUPDF_AVAILABLE,
     UNSTRUCTURED_AVAILABLE

--- tests/test_core.py
+++ tests/test_core.py
@@ -4,9 +4,6 @@
 
 import json
 import logging
-from datetime import datetime
-from typing import Any, Dict, List, Optional, Union
-from unittest.mock import AsyncMock, Mock, patch
 
 import pytest
 
@@ -21,7 +18,7 @@
     VectorStoreError,
 )
 from isa_superapp.core.logging_config import get_logger, setup_logging
-from isa_superapp.core.metrics import MetricsCollector, MetricType
+from isa_superapp.core.metrics import MetricsCollector
 from isa_superapp.core.security import SecurityManager, decrypt_data, encrypt_data
 from isa_superapp.core.validation import validate_config, validate_document
 

--- tests/unit/test_retrieval.py
+++ tests/unit/test_retrieval.py
@@ -6,12 +6,11 @@
 including hybrid search, reranking, and context assembly.
 """
 
-from typing import Any, Dict, List, Optional
-from unittest.mock import AsyncMock, MagicMock, patch
+from unittest.mock import MagicMock
 
 import pytest
 
-from tests.utils import MockFactory, TestDataGenerator, TestValidators
+from tests.utils import TestDataGenerator
 
 
 class TestRetrievalEngine:

--- src/agent_core/tests/test_database_pooling.py
+++ src/agent_core/tests/test_database_pooling.py
@@ -2,8 +2,8 @@
 import time
 import threading
 import statistics
-from unittest.mock import Mock, patch, MagicMock
-from sqlalchemy import create_engine, text
+from unittest.mock import Mock, patch
+from sqlalchemy import text
 from sqlalchemy.pool import QueuePool
 from src.database_manager import DatabaseConnectionManager, get_db_manager
 

--- src/dmn/dmn_parser.py
+++ src/dmn/dmn_parser.py
@@ -8,7 +8,7 @@
 import json
 import yaml
 import logging
-from typing import Dict, List, Any, Optional, Union
+from typing import Dict, List, Any, Union
 from pathlib import Path
 import xml.etree.ElementTree as ET
 

--- src/docs_provider/pymupdf_processor.py
+++ src/docs_provider/pymupdf_processor.py
@@ -25,7 +25,7 @@
     UNSTRUCTURED_AVAILABLE = False
     logging.warning("Unstructured not available. Install with: pip install unstructured[pdf]")
 
-from .base import DocsProvider, DocsSnippet, ProviderResult
+from .base import DocsProvider, ProviderResult
 
 
 @dataclass

--- src/agent_core/streaming_processor.py
+++ src/agent_core/streaming_processor.py
@@ -1,8 +1,4 @@
-import re
 import logging
-from typing import List, Dict, Any, Optional, Tuple
-from dataclasses import dataclass
-from .llm_client import get_openrouter_free_client
 
 # Import GS1 parser
 try:

--- tests/test_security.py
+++ tests/test_security.py
@@ -5,7 +5,6 @@
 import pytest
 from unittest.mock import Mock, patch
 from fastapi.testclient import TestClient
-import json
 
 from src.api_server import app
 from src.auth import User, UserRole, create_access_token

--- src/taxonomy/efrag_esrs_loader.py
+++ src/taxonomy/efrag_esrs_loader.py
@@ -7,7 +7,7 @@
 
 import logging
 from pathlib import Path
-from typing import Dict, List, Optional, Any, Union
+from typing import Dict, List, Optional, Any
 from dataclasses import dataclass
 from datetime import datetime
 import json
@@ -15,7 +15,7 @@
 
 from ..database_manager import DatabaseConnectionManager
 
-from .models import ESRSTaxonomy as ESRSTaxonomyModel, TaxonomyElement as TaxonomyElementModel, TaxonomyTable as TaxonomyTableModel, TaxonomyLoadLog, create_taxonomy_tables
+from .models import ESRSTaxonomy as ESRSTaxonomyModel, TaxonomyElement as TaxonomyElementModel, TaxonomyTable as TaxonomyTableModel, TaxonomyLoadLog
 
 
 @dataclass

--- src/agent_core/memory/rag_store.py
+++ src/agent_core/memory/rag_store.py
@@ -1,11 +1,10 @@
 import hashlib
-import json
 import logging
 import re
 import threading
 from datetime import datetime, timezone
 from pathlib import Path
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import Any, Dict, List, Optional, Tuple
 
 import chromadb
 from chromadb.utils import embedding_functions
@@ -277,7 +276,7 @@
                         extra_metadata = doc.get("extra_metadata", {})
 
                         if not text or not source or not doc_id:
-                            logging.warning(f"Skipping invalid document in batch: missing required fields")
+                            logging.warning("Skipping invalid document in batch: missing required fields")
                             continue
 
                         # Chunk the text

--- tests/test_neo4j_gds.py
+++ tests/test_neo4j_gds.py
@@ -16,11 +16,9 @@
 import pytest
 import unittest.mock as mock
 from datetime import datetime, timezone
-from typing import Dict, Any, List
 
 from src.neo4j_gds_schema import (
-    SupplyChainGraphSchema, NodeType, RelationshipType,
-    GraphNode, GraphRelationship
+    SupplyChainGraphSchema, NodeType, RelationshipType
 )
 from src.neo4j_gds_client import Neo4jGDSClient, Neo4jConfig, GDSConfig
 from src.neo4j_gds_ingestion import (

--- src/dmn/dmn_manager.py
+++ src/dmn/dmn_manager.py
@@ -13,7 +13,7 @@
 
 from .dmn_table import DMNTable, DecisionTable, DMNExecutionContext, DMNExecutionResult
 from .dmn_engine import DMNEngine
-from .dmn_parser import DMNParser, DMNParseError
+from .dmn_parser import DMNParser
 
 
 @dataclass

--- src/langgraph_agents/risk_assessor.py
+++ src/langgraph_agents/risk_assessor.py
@@ -6,7 +6,7 @@
 """
 
 import logging
-from typing import Dict, List, Any, Optional, TypedDict
+from typing import Dict, List, Any, TypedDict
 
 try:
     from langgraph.graph import StateGraph, END

--- src/langgraph_agents/document_analyzer.py
+++ src/langgraph_agents/document_analyzer.py
@@ -6,7 +6,7 @@
 """
 
 import logging
-from typing import Dict, List, Any, Optional, TypedDict
+from typing import Dict, List, Any, TypedDict
 
 try:
     from langgraph.graph import StateGraph, END

--- src/taxonomy/models.py
+++ src/taxonomy/models.py
@@ -6,7 +6,6 @@
 """
 
 from datetime import datetime
-from typing import List, Optional, Dict, Any
 from sqlalchemy import Column, Integer, String, Text, DateTime, JSON, ForeignKey, Table
 from sqlalchemy.orm import relationship, declarative_base
 from sqlalchemy.ext.declarative import declarative_base

--- src/dmn/dmn_table.py
+++ src/dmn/dmn_table.py
@@ -5,10 +5,9 @@
 tables used in compliance rules automation.
 """
 
-from typing import Dict, List, Any, Optional, Union, Callable
+from typing import Dict, List, Any, Optional
 from dataclasses import dataclass, field
 from enum import Enum
-import logging
 
 
 class HitPolicy(Enum):

--- src/dmn/dmn_compliance_integration.py
+++ src/dmn/dmn_compliance_integration.py
@@ -6,7 +6,7 @@
 """
 
 import logging
-from typing import Dict, List, Any, Optional, Union
+from typing import Dict, List, Any, Optional
 from dataclasses import dataclass
 
 from .dmn_manager import DMNManager, DMNManagerConfig

--- src/dmn/dmn_engine_fixed.py
+++ src/dmn/dmn_engine_fixed.py
@@ -7,14 +7,10 @@
 
 import time
 import logging
-from typing import Dict, List, Any, Optional, Union, Callable
-from dataclasses import dataclass
-import operator
-import re
+from typing import Dict, List, Any, Optional, Union
 
 from .dmn_table import (
-    DMNTable, DecisionTable, Rule, InputClause, OutputClause,
-    HitPolicy, BuiltinAggregator, ExpressionLanguage,
+    DecisionTable, Rule, HitPolicy, BuiltinAggregator, ExpressionLanguage,
     DMNExecutionContext, DMNExecutionResult
 )
 

--- src/taxonomy/test_taxonomy_loader.py
+++ src/taxonomy/test_taxonomy_loader.py
@@ -11,11 +11,9 @@
 import json
 import os
 import tempfile
-from pathlib import Path
 from datetime import datetime
 
-from .efrag_esrs_loader import EFRAGESRSTaxonomyLoader, ESRSTaxonomy, TaxonomyElement, TaxonomyTable
-from .models import create_taxonomy_tables
+from .efrag_esrs_loader import EFRAGESRSTaxonomyLoader
 from ..database_manager import get_db_manager
 
 

--- src/config/geospatial_config.py
+++ src/config/geospatial_config.py
@@ -6,7 +6,7 @@
 """
 
 import os
-from typing import Dict, Any, Optional
+from typing import Dict, Any
 
 
 class GeospatialConfig:

--- src/config/gs1_config.py
+++ src/config/gs1_config.py
@@ -7,7 +7,7 @@
 
 import os
 from pathlib import Path
-from typing import Dict, List, Optional
+from typing import Dict, List
 from dataclasses import dataclass
 
 @dataclass

--- src/dmn/dmn_engine.py
+++ src/dmn/dmn_engine.py
@@ -7,14 +7,10 @@
 
 import time
 import logging
-from typing import Dict, List, Any, Optional, Union, Callable
-from dataclasses import dataclass
-import operator
-import re
+from typing import Dict, List, Any, Optional
 
 from .dmn_table import (
-    DMNTable, DecisionTable, Rule, InputClause, OutputClause,
-    HitPolicy, BuiltinAggregator, ExpressionLanguage,
+    DecisionTable, Rule, HitPolicy, BuiltinAggregator, ExpressionLanguage,
     DMNExecutionContext, DMNExecutionResult
 )
 

--- src/database_manager.py
+++ src/database_manager.py
@@ -20,7 +20,7 @@
 from sqlalchemy.engine import Engine
 from sqlalchemy.orm import sessionmaker, Session
 from sqlalchemy.pool import QueuePool
-from .cache.multi_level_cache import get_multilevel_cache, MultiLevelCache
+from .cache.multi_level_cache import get_multilevel_cache
 
 logger = logging.getLogger(__name__)
 

--- src/vc_data_model_verifier.py
+++ src/vc_data_model_verifier.py
@@ -8,7 +8,7 @@
 import json
 import logging
 from pathlib import Path
-from typing import Dict, List, Optional, Any, Union, Set
+from typing import Dict, List, Optional, Any, Union
 from dataclasses import dataclass
 from enum import Enum
 

--- src/supplier_attestation_vc.py
+++ src/supplier_attestation_vc.py
@@ -6,7 +6,7 @@
 """
 
 from dataclasses import dataclass, asdict
-from typing import Dict, List, Optional, Any, Union
+from typing import Dict, List, Optional, Any
 from datetime import datetime, timezone, timedelta
 from enum import Enum
 import json

--- src/cache/multi_level_cache.py
+++ src/cache/multi_level_cache.py
@@ -16,13 +16,12 @@
 
 import hashlib
 import json
-import os
 import pickle
 import threading
 import time
 from collections import OrderedDict
 from pathlib import Path
-from typing import Any, Dict, Optional, Union, Callable
+from typing import Any, Dict, Optional
 import redis
 
 

--- src/neo4j_gds_analytics.py
+++ src/neo4j_gds_analytics.py
@@ -16,16 +16,14 @@
 """
 
 import logging
-import time
-from typing import Dict, List, Any, Optional, Tuple, Union
+from typing import Dict, List, Any, Optional
 from datetime import datetime, timedelta
 from dataclasses import dataclass
 from enum import Enum
 import pandas as pd
-import numpy as np
 
 from .neo4j_gds_client import Neo4jGDSClient, get_gds_client
-from .neo4j_gds_schema import SupplyChainGraphSchema, NodeType, RelationshipType
+from .neo4j_gds_schema import SupplyChainGraphSchema
 
 logger = logging.getLogger(__name__)
 

Would fix 469 errors (191 additional fixes available with `--unsafe-fixes`).
--- src/GS1DigitalLinkResolverTestSuite.js	2025-09-16 20:23:36
+++ temp_formatted.js	2025-09-16 20:23:36
@@ -1,1834 +1 @@
-const outputElement = 'gs1ResolverTests'; // Set this to the id of the element in the HTML document where you want the output to go
-
-// We'll run a series of largely asynchronous discrete tests, each of which is
-// defined within a JSON object as follows. These objects are sent to the 'recordResult'
-// function when first defined and then again after the test has been completed. This
-// is what creates the effect of red squares appearing and then, all being well, turning
-// green as the test is passed.
-
-const resultProps = {
-  id: '', //An id for the test
-  test: '', // conformance statement from spec
-  status: 'fail', // (pass|fail|warn), default is fail
-  msg: '', // Displayed to the end user
-  url: '', // The URL we're going to fetch
-  headers: {}, // Ready for any headers we want to set
-};
-
-// Where possible, we'll use JavaScript's Fetch function but this is insufficient for most of
-//  the tests we need to run. In those cases, we'll need to use a PHP script that executes the
-// request and sends the response back as a JSON object.
-
-// const testUri = 'http://localhost:8000/test-suites/resolver/1.0.0/tester.php';
-// const testUri = 'https://ref.gs1.org/test-suites/resolver/1.0.0/tester.php';
-const testUri = 'https://philarcher.org/gs1/tester.php';
-
-// We'll make use of two JSON schemas
-const resolverDescriptionFileSchema =
-  'https://ref.gs1.org/standards/resolver/description-file-schema';
-const gs1LinksetSchema =
-  'https://gs1.github.io/linkset/gs1-linkset-schema.json';
-
-// const RabinRegEx = /^(([^:\/?#]+):)?(\/\/((([^\/?#]*)@)?([^\/?#:]*)(:([^\/?#]*))?))?([^?#]*)(\?([^#]*))?(#(.*))?/;
-const RabinRegEx =
-  /^((https?):)(\/\/((([^\/?#]*)@)?([^\/?#:]*)(:([^\/?#]*))?))?([^?#]*)(\?([^#]*))?(#(.*))?/i; // As above but specifically for HTTP(s) URIs
-// (see https://www.w3.org/TR/powder-grouping/#rabinsRegEx for the origin of this regex by Jo Rabin)
-// gives [2] scheme, [4] domain,[9] port, [10] path, [12] query, [14] fragment
-
-// This is 'RE1' from the DL 1.2 spec but with added case insensitive flag
-const plausibleDlURI =
-  /^https?:(\/\/((([^\/?#]*)@)?([^\/?#:]*)(:([^\/?#]*))?))?([^?#]*)(((\/(01|gtin|8006|itip|8013|gmn|8010|cpid|414|gln|417|party|8017|gsrnp|8018|gsrn|255|gcn|00|sscc|253|gdti|401|ginc|402|gsin|8003|grai|8004|giai)\/)(\d{4}[^\/]+)(\/[^/]+\/[^/]+)?[/]?(\?([^?\n]*))?(#([^\n]*))?))/i;
-// And this is 'RE2' also with added case insensitive flag ('RE3' that attempts to look for a compressed DL URI is not used in the test suite).
-const plausibleDlURINoAlphas =
-  /^https?:(\/\/((([^\/?#]*)@)?([^\/?#:]*)(:([^\/?#]*))?))?([^?#]*)(((\/(01|8006|8013|8010|414|417|8017|8018|255|00|253|401|402|8003|8004)\/)(\d{4}[^\/]+)(\/[^/]+\/[^/]+)?[/]?(\?([^?\n]*))?(#([^\n]*))?))/i;
-
-// We're going to be checking that link types found are in the ratified list
-const linkTypeListSource = 'https://ref.gs1.org/voc/data/linktypes';
-
-// We will be trying to get the linkset by various methods. We will need to keep a note
-// of the method(s) that work
-
-let linksetGetMethods = {};
-
-const modelLinkset = {
-  linkset: [
-    {
-      anchor: 'https://id.gs1.org/01/09506000164908',
-      itemDescription: 'Crew neck white t-shirt',
-      'https://ref.gs1.org/voc/certificationInfo': [
-        {
-          href: 'https://ref.gs1.org/tools/demo/2024retail/certificates',
-          title: 'Certificates',
-          type: 'text/html',
-        },
-        {
-          href: 'https://www.oeko-tex.com/v/07.BH.52767',
-          title: 'OEKO-TEX certificate',
-          type: 'text/html',
-          hreflang: ['fr'],
-        },
-        {
-          href: 'https://supplier.roadmaptozero.com/flp/certificate/verify/5067/95ac8a1acf238e6ae40d761400687fac',
-          title: 'Roadmap to Zero certificate',
-          type: 'text/html',
-        },
-        {
-          href: 'https://supplier.roadmaptozero.com/flp/certificate/verify/5067/95ac8a1acf238e6ae40d761400687fac',
-          title: 'Roadmap to Zero certificate',
-          type: 'text/html',
-        },
-        {
-          href: 'https://certificate.example/001',
-          title: 'Another certificate',
-          type: 'application/pdf',
-        },
-        {
-          href: 'https://certificate.example/002',
-          title: 'Another certificate',
-          type: 'application/pdf',
-          hreflang: ['en'],
-        },
-        {
-          href: 'https://certificate.example/003',
-          title: 'Another certificate',
-          type: 'application/pdf',
-          hreflang: ['en'],
-          context: ['LK'],
-        },
-      ],
-      'https://ref.gs1.org/voc/homepage': [
-        {
-          href: 'https://ref.gs1.org/tools/demo/2024retail/',
-          title: 'Menu',
-          type: 'text/html',
-          hreflang: ['en'],
-        },
-      ],
-      'https://ref.gs1.org/voc/instructions': [
-        {
-          href: 'https://ref.gs1.org/tools/demo/2024retail/recycling',
-          title: 'Recycling',
-          type: 'text/html',
-          hreflang: ['en'],
-        },
-      ],
-      'https://ref.gs1.org/voc/pip': [
-        {
-          href: 'https://ref.gs1.org/tools/demo/2024retail/pip',
-          title: 'Product Info',
-          type: 'text/html',
-          hreflang: ['en'],
-        },
-      ],
-      'https://ref.gs1.org/voc/sustainabilityInfo': [
-        {
-          href: 'https://ref.gs1.org/tools/demo/2024retail/en/sustainability',
-          title: 'Sustainability',
-          type: 'text/html',
-          hreflang: ['en'],
-        },
-        {
-          href: 'https://ref.gs1.org/tools/demo/2024retail/fr/sustainability',
-          title: 'Sustainability',
-          type: 'text/html',
-          hreflang: ['fr'],
-        },
-      ],
-      'https://ref.gs1.org/voc/traceability': [
-        {
-          href: 'https://ref.gs1.org/tools/demo/2024retail/track-and-trace',
-          title: 'Track and Trace',
-          type: 'text/html',
-        },
-        {
-          href: 'https://ref.gs1.org/tools/demo/2024retail/track-and-trace',
-          title: 'Track and Trace',
-          type: 'text/html',
-        },
-      ],
-      'https://ref.gs1.org/voc/defaultLink': [
-        {
-          href: 'https://ref.gs1.org/tools/demo/2024retail/',
-          title: 'Menu',
-        },
-      ],
-    },
-    {
-      anchor: 'https://id.gs1.org/01/09506000164908/21/1234',
-      itemDescription: 'Crew neck white t-shirt, serial number 1234',
-      'https://ref.gs1.org/voc/dpp': [
-        {
-          href: 'https://example.com/dpp/7132mlkG',
-          title: 'DPP',
-          type: 'application/vc',
-        },
-      ],
-    },
-  ],
-};
-
-// Global variables
-let resultsArray = [];
-
-// ***************************************************************************
-// This is the main function that takes a Digital Link URI as input and creates the output.
-// ***************************************************************************
-
-const testDL = async (dl) => {
-  clearGrid();
-  // We start with some basic tests by simply inspecting the incoming URI
-  // No external calls are made at this stage.
-
-  let domain; // We'll need to make tests on the domain name in the DL URI
-
-  // First of all - is the incoming string a URL at all?
-  let isURL = Object.create(resultProps);
-  isURL.id = 'isURL';
-  isURL.test =
-    'Not listed as a conformance criterion but the DL URI must be a valid URL';
-  isURL.msg = 'Given GS1 Digital Link URI is not a valid URL';
-  recordResult(isURL); // The default message is sent to the output. We'll update it if the test is passed.
-
-  // While we're at it, we'll set up the results object for whether it's a valid DL URI or not
-  let validDL = Object.create(resultProps);
-  validDL.id = 'validDL';
-  validDL.test = 'Given input must be a GS1 Digital Link URI';
-  validDL.msg =
-    'Given input is not a valid GS1 Digital link URI, no further testing is possible';
-  recordResult(validDL);
-
-  // And whether it's using HTTPS or not
-  let isHttps = Object.create(resultProps);
-  isHttps.id = 'isHttps';
-  isHttps.test = 'SHALL support HTTP Over TLS (HTTPS)';
-  isHttps.msg = 'Given GS1 Digital Link URI uses a scheme other than HTTPS';
-  isHttps.status = 'warn';
-  recordResult(isHttps);
-
-  // We will tolerate leading and training spaces but not spaces within the URL
-  dl = dl.replace(/(^\s+|\s+$)/g, ''); // Remove leading and trailing spaces
-  // console.log('Given GS1 Digital Link URI is "' + dl + '"');
-
-  // We'll split the URI using Jo Rabin's regex
-  let UriElements = dl.match(RabinRegEx);
-  // If the input matches the regex, it's an http or https URI
-  if (UriElements) {
-    let scheme = UriElements[2].toLowerCase();
-    domain = UriElements[4]; // Sets this important variable
-    if (
-      (scheme === 'http' || scheme === 'https') &&
-      domain.indexOf('.') !== -1
-    ) {
-      isURL.msg = 'Given GS1 Digital Link URI is a valid URL';
-      isURL.status = 'pass';
-      recordResult(isURL);
-      // At this point we probably have a URL and we have its various elements,
-    } // End is it a URL
-    if (scheme === 'https') {
-      isHttps.msg = 'Given GS1 Digital Link URI defines HTTPS as its scheme';
-      isHttps.status = 'pass';
-      recordResult(isHttps);
-    }
-  } // End is it a URI
-
-  let plausibleDL;
-  // if isHttps.status is pass or warn then we have a URL and we can probe further
-  if (isHttps.status !== 'fail') {
-    plausibleDL = Object.create(resultProps);
-    plausibleDL.id = 'plausibleDL';
-    plausibleDL.test =
-      'Following GS1 Digital Link: URI syntax, we can check whether a URL plausibly is, or definitely is not, a DL URI';
-    plausibleDL.msg =
-      'Given URL does not conform to GS1 Digital Link URI syntax, no further tests are possible';
-    plausibleDL.status = 'fail';
-    // Let's check that it is a plausible DL URI using the regular expression method in the DL URI spec
-    if (plausibleDlURINoAlphas.test(dl)) {
-      plausibleDL.status = 'pass';
-      plausibleDL.msg =
-        'URL under test plausibly is a GS1 Digital Link URI (uncompressed)';
-    } else if (plausibleDlURI.test(dl)) {
-      plausibleDL.status = 'warn';
-      plausibleDL.msg =
-        'URL under test plausibly is a GS1 Digital Link URI but is using convenience alphas that were deprecated in version 1.2.0 and removed entirley in version 1.3.0 published in November 2022';
-    }
-
-    recordResult(plausibleDL);
-  }
-  // So now if plausibleDL.status is pass or warn, then we can pass it to the toolkit for a full check
-  if (plausibleDL.status !== 'fail') {
-    let gs1dlt = new GS1DigitalLinkToolkit(); // We'll need to use the GS1 Digital Link toolkit
-    try {
-      let gs1Array = gs1dlt.extractFromGS1digitalLink(dl);
-      // You can get here with a variety of URLs including just a domain name and /gtin etc.
-      // So we need to check further Object returned has a GS1 object within it.
-      // Test for that using Mark's code unless and until we can switch to the BSR
-
-      if (gs1dlt.buildStructuredArray(gs1Array.GS1).identifiers.length === 1) {
-        validDL.status = 'pass';
-        validDL.msg = 'Given input is a valid GS1 Digital Link URI';
-        recordResult(validDL);
-      }
-    } catch (err) {
-      console.log(
-        'Error when extracting keys from given DL. Message is ' + err
-      );
-    }
-  }
-
-  // If validDL.status is pass, we're good to go.
-  if (validDL.status === 'pass') {
-    // We'll call a series of functions rather than putting everything here
-    // They return an object that normally goes into the async fetch array
-
-    TLSCheck(domain).then(); // This one doesn't push to the array
-    rdFileCheck(domain).then(); // Nor this one, so we don't need to wait either for them
-    //We'll wait for these run tests
-    await runTest(checkHttpVersion(domain));
-    await runTest(headerBasedChecks(dl));
-    await runTest(errorCodeChecks(dl));
-    await runTest(trailingSlashCheck(dl));
-    //await runTest(compressionChecks(dl, domain, gs1dlt));
-    await runTest(testQuery(dl));
-    await runTest(linkTypeIslinkset(dl, true));
-    await runTest(ltWithAcceptHeader(dl, true));
-    await runTest(fetchAndValidateTheLinkset(dl));
-    //await runTest(linksetJsonldHeaderTest(dl));
-    await runTest(testFor404(dl));
-    await resultSummary();
-  }
-  rotatingCircle(false);
-  // End if validDL.status=='pass'
-};
-
-const TLSCheck = async (domain) => {
-  // This is designed to make sure that the server is available over TLS (i.e. using
-  // https works, even if the given DL is http) It does not handle a JSON
-  // response and therefore we don't use the promises array
-  let tlsOK = Object.create(resultProps);
-  tlsOK.id = 'tlsOK';
-  tlsOK.test = 'SHALL support HTTP Over TLS (HTTPS)';
-  tlsOK.msg = 'Resolver does not support HTTP over TLS';
-  recordResult(tlsOK);
-
-  try {
-    let response = await fetch('https://' + domain, {
-      method: 'HEAD',
-      mode: 'no-cors',
-    });
-    // console.log(`response status is ${response.status}`)
-    if (response.status >= 0) {
-      // status is usually 0 for a site that supports https, I think 'cos we're
-      //  using non-cors mode. This test could really do with improving
-      tlsOK.msg = 'Confirmed that server supports HTTP over TLS';
-      tlsOK.status = 'pass';
-    }
-    recordResult(tlsOK);
-  } catch (error) {
-    console.log(
-      'TLSCheck() Error: There has been a problem with your fetch operation when checking for TLS support: ',
-      error.message
-    );
-  }
-  return tlsOK;
-};
-
-const rdFileCheck = async (domain) => {
-  // Checking that the Resolver Description File is available. Also want to check a few things about it.
-
-  let rdFile = Object.create(resultProps);
-  rdFile.id = 'rdFile';
-  rdFile.test =
-    'SHALL provide a resolver description file at /.well-known/gs1resolver';
-  rdFile.msg = 'Resolver Description File not found';
-  recordResult(rdFile);
-
-  try {
-    let testRequest = new Request(
-      'https://' + domain + '/.well-known/gs1resolver',
-      {
-        method: 'get',
-        mode: 'cors',
-        redirect: 'follow',
-        headers: new Headers({
-          Accept: 'application/json',
-        }),
-      }
-    );
-    let response = await fetch(testRequest);
-    let data = await response.json();
-    const schemaTestResult = await doesJSONSchemaPass(
-      data,
-      resolverDescriptionFileSchema
-    );
-    //if (
-    //    data['supportedPrimaryKeys'] &&
-    //    data['resolverRoot'].match(/^((https?):)(\/\/((([^\/?#]*)@)?([^\/?#:]*)(:([^\/?#]*))?))?([^?#]*)(\?([^#]*))?(#(.*))?/) &&
-    //    schemaTestResult.testResult
-    //)
-    if (schemaTestResult.testResult) {
-      rdFile.msg =
-        'Resolver description file found with at least minimum required data and is valid against the official GS1 schema';
-      rdFile.status = 'pass';
-    } else if (response.ok) {
-      rdFile.msg =
-        'Resolver description file found but does not validate against the official GS1 schema. Error(s): ' +
-        schemaTestResult.errors.join(', ');
-    }
-    //else
-    //{
-    //    rdFile.msg = 'Resolver description file found but minimum required data not found';
-    //}
-    recordResult(rdFile);
-  } catch (error) {
-    console.log(
-      'rdFileCheck() Error: There has been a problem with your fetch operation: ',
-      error.message
-    );
-  }
-  return rdFile.status;
-};
-
-const checkHttpVersion = (domain) => {
-  let httpVersion = Object.create(resultProps);
-  httpVersion.id = 'httpVersion';
-  httpVersion.test = 'SHALL support HTTP 1.1 (or higher)';
-  httpVersion.status = 'warn';
-  httpVersion.msg =
-    "HTTP version not detected. If other tests passed, it's probably OK";
-  httpVersion.url =
-    testUri +
-    '?test=getHTTPversion&testVal=' +
-    encodeURIComponent(`https://${domain}`);
-  recordResult(httpVersion);
-  httpVersion.process = async (data) => {
-    let r = parseFloat(
-      data.result.toUpperCase().replace('HTTP', '').replace('/', '')
-    );
-    if (r && r >= 1.1) {
-      httpVersion.status = 'pass';
-      httpVersion.msg = 'Server at ' + domain + ' supports HTTP ' + r;
-    }
-    recordResult(httpVersion);
-  };
-
-  return httpVersion;
-};
-
-const headerBasedChecks = (dl) => {
-  // We'll perform a number of checks based on the headers returned from checking the DL directly
-
-  let corsCheck = Object.create(resultProps);
-  corsCheck.id = 'corsCheck';
-  corsCheck.test = 'SHALL support CORS';
-  corsCheck.msg = 'CORS headers not detected';
-  recordResult(corsCheck);
-
-  // *************** Need to look at the link header
-  let linkOnRedirect = Object.create(resultProps);
-  linkOnRedirect.id = 'linkOnRedirect';
-  linkOnRedirect.test =
-    'SHOULD expose the direct link to the linkset in an HTTP Link header when redirecting.';
-  linkOnRedirect.status = 'warn';
-  linkOnRedirect.msg = 'No link to the linkset detected when redirecting';
-  recordResult(linkOnRedirect);
-
-  let legacyLinkHeaders = Object.create(resultProps);
-  legacyLinkHeaders.id = 'legacyLinkHeaders';
-  legacyLinkHeaders.test =
-    'The inclusion of all possible links in the HTTP Link header was deprecated in version 1.0.0 of the GS1-Conformant resolver standard, February 2024';
-  legacyLinkHeaders.status = 'pass'; // This is unusual - we're setting this pass and will change to warn if we find a reason
-  legacyLinkHeaders.msg = 'No unnecessary links found in HTTP Link header';
-  recordResult(legacyLinkHeaders);
-
-  let methodsCheck = Object.create(resultProps);
-  methodsCheck.id = 'methodsCheck';
-  methodsCheck.test =
-    'SHALL support HTTP 1.1 (or higher) GET, HEAD and OPTIONS requests.';
-  methodsCheck.msg = 'At least one of GET, HEAD or OPTIONS not detected';
-  recordResult(methodsCheck);
-
-  let u = stripQueryStringFromURL(dl);
-  corsCheck.url =
-    testUri + '?test=getAllHeaders&testVal=' + encodeURIComponent(u);
-  // console.log(`Looking at ${testUri + '?test=getAllHeaders&testVal=' + encodeURIComponent(u)}`);
-  corsCheck.process = async (data) => {
-    //console.log(`What came back was ${JSON.stringify(data.result)}`);
-    if (
-      data.result['Access-Control-Allow-Origin'] ||
-      data.result['access-control-allow-origin']
-    ) {
-      // That's probably enough tbh
-      corsCheck.status = 'pass';
-      corsCheck.msg = 'CORS headers detected';
-      recordResult(corsCheck);
-    }
-    // Need to handle case insensitivity - some servers use title case, others lower case
-    if (
-      data.result['Access-Control-Allow-Origin'] !== undefined &&
-      data.result['access-control-allow-methods'] === undefined
-    ) {
-      data.result['access-control-allow-methods'] =
-        data.result['Access-Control-Allow-Methods'];
-    }
-
-    if (
-      typeof data.result['access-control-allow-methods'] === 'string' &&
-      data.result['access-control-allow-methods'].indexOf('GET') > -1 &&
-      data.result['access-control-allow-methods'].indexOf('HEAD') > -1 &&
-      data.result['access-control-allow-methods'].indexOf('OPTIONS') > -1
-    ) {
-      // We have our three allowed methods
-      methodsCheck.msg =
-        'GET, HEAD an OPTIONS methods declared to be supported';
-      methodsCheck.status = 'pass';
-      recordResult(methodsCheck);
-    }
-
-    // Handling case-insensitivity of 'Link/link' header
-    let linkHeader = data.result['Link'];
-    if (!linkHeader) {
-      linkHeader = data.result['link'];
-    }
-    if (linkHeader) {
-      // we have a link header.
-      // We just want to make sure that there's a link to the linkset.
-      // This means a link with a rel value of linkset
-      // If there is more than one link in the header, we can see if they're still
-      // including all the links from the linkset. If so, we can warn and say that this
-      // is now deprecated.
-
-      // console.log(`Link header is ${linkHeader}`);
-      let allLinks = linkHeader.split('",');
-      for (let i in allLinks) {
-        if (!allLinks.hasOwnProperty(i)) {
-          continue;
-        }
-        allLinks[i] += '"';
-      }
-
-      // console.log(`All links is ${allLinks}`);
-      // So we have an array, each item in a link with all its attributes
-      // We just need to find a link with rel="linkset"
-      for (link in allLinks) {
-        // console.log(`Looking at a link header ${allLinks[link]}`)
-        if (allLinks[link].indexOf('rel="linkset"') > -1) {
-          linkOnRedirect.status = 'pass';
-          linkOnRedirect.msg = 'Link to the linkset detected when redirecting';
-          recordResult(linkOnRedirect);
-        }
-        if (allLinks[link].indexOf('rel="gs1:') > -1) {
-          // All we're looking for is a link with an @rel value that includes a gs1: CURIE
-          legacyLinkHeaders.status = 'warn';
-          legacyLinkHeaders.msg = 'Unnecessary links found in HTTP Link header';
-          recordResult(legacyLinkHeaders);
-        }
-      }
-    }
-  };
-  return corsCheck;
-};
-
-const errorCodeChecks = (dl) => {
-  // ******* Test for appropriate use of 400
-  let reportWith400 = Object.create(resultProps);
-  reportWith400.id = 'reportWith400';
-  reportWith400.test =
-    'SHALL extract and syntactically validate the URI and report errors with an HTTP response code of 400';
-  reportWith400.msg =
-    'Non-conformant GS1 Digital Link URI not reported with 400 error';
-  recordResult(reportWith400);
-
-  // ********** Also test not using 200 to report errors
-  let noErrorWith200 = Object.create(resultProps);
-  noErrorWith200.id = 'noErrorWith200';
-  noErrorWith200.test =
-    'A GS1 conformant resolver SHALL NOT use a 200 OK response code with a resource that indicates an error condition';
-  noErrorWith200.msg = 'Error reported with 200 OK';
-  recordResult(noErrorWith200);
-
-  // Let's create an error - we know we have a valid DL so we'll mess it up by adding /foo to the end
-  reportWith400.url =
-    testUri +
-    '?test=getAllHeaders&testVal=' +
-    encodeURIComponent(stripQueryStringFromURL(dl) + '/foo');
-  reportWith400.process = async (data) => {
-    let httpResponseCode = data.result['httpCode'];
-    //console.log(`We have a response code of ${data.result['0']}`);
-    if (httpResponseCode === 400) {
-      reportWith400.msg =
-        'Non-conformant GS1 Digital Link URI correctly reported with 400 error';
-      reportWith400.status = 'pass';
-    } else {
-      reportWith400.msg = `Non-conformant GS1 Digital Link URI wrongly reported with HTTP ${httpResponseCode} error`;
-    }
-    recordResult(reportWith400);
-    if (httpResponseCode !== 200) {
-      noErrorWith200.msg = `Error was correctly reported with a non-200 OK response code of HTTP ${httpResponseCode}`;
-      noErrorWith200.status = 'pass';
-      recordResult(noErrorWith200);
-    }
-  };
-  return reportWith400;
-};
-
-const trailingSlashCheck = (dl) => {
-  // Need to create a URI with and without a trailing slash
-  // We can dispense with any query string and create version with no trailing slash, then append slash to make the
-  // other
-  let noSlash = stripQueryStringFromURL(dl);
-  if (noSlash.lastIndexOf('/') + 1 === noSlash.length) {
-    noSlash = noSlash.substring(0, noSlash.lastIndexOf('/'));
-  }
-  let slash = noSlash + '/';
-
-  let trailingSlash = Object.create(resultProps);
-  trailingSlash.id = 'trailingSlash';
-  trailingSlash.test =
-    'SHOULD tolerate trailing slashes at the end of GS1 Digital Link URIs, i.e. the resolver SHOULD NOT fail if one is present';
-  trailingSlash.msg =
-    'Resolver responds differently with or without a trailing slash';
-  trailingSlash.status = 'warn'; // This is a SHOULD not a SHALL so warn is as strong as we should be
-  recordResult(trailingSlash);
-
-  trailingSlash.url =
-    testUri + '?test=getAllHeaders&testVal=' + encodeURIComponent(noSlash);
-  trailingSlash.process = async (data) => {
-    try {
-      let slashRequest = new Request(
-        testUri + '?test=getAllHeaders&testVal=' + encodeURIComponent(slash),
-        {
-          method: 'get',
-          mode: 'cors',
-        }
-      );
-      let response = await fetch(slashRequest);
-      let slashJSON = await response.json();
-      if (
-        data.result.httpCode === slashJSON.result.httpCode &&
-        data.result.location === slashJSON.result.location
-      ) {
-        trailingSlash.status = 'pass';
-        trailingSlash.msg =
-          'Response from server with and without trailing slash is identical';
-        recordResult(trailingSlash); // No need to update if we didn't get here
-      }
-    } catch (error) {
-      console.log(
-        'trailingSlashCheck() Error: There has been a problem with your fetch operation for ' +
-          trailingSlash.url +
-          ': ',
-        error.message
-      );
-    }
-  };
-  return trailingSlash;
-};
-
-// Need to do lots of tests with the linkset
-// Starting with "can I get it?"
-
-// Ask for linkset using linkType=linkset, If something comes back, hang on to it.
-// Ask for linkset using linkType=all - if anything somes back it should be the same as previous. Warn of deprecation
-// Don't set Accept header with those two - should be HTML
-// Repeat with Accept header set to application/json - should be JSON, doesn't have to be in the linkset formet
-// Ask for linkset using Accept header of application/linkset+json without linkType param set (shoud be the same)
-// Check for presence of Vary header
-// In previous, check for presence of link header to context file by looking at value of @rel, should be the long w3.org URL
-// However we got it, validate linkset against the schema
-// If valid, test default link.
-// If there's a defaultLinkMulti, work through those
-// Now work through all the other links by asking for them explicitly - should redirect to the target
-// Test for multiple links with exactly the same metadata - which should return a 300
-
-const linkTypeIslinkset = (dl, firstRun) => {
-  // Check that setting linkType to linkset does not redirect or cause an error
-  let ltLinksetNoRedirect = Object.create(resultProps);
-  ltLinksetNoRedirect.id = 'ltLinksetNoRedirect';
-  ltLinksetNoRedirect.test =
-    'On receiving a request for the linkset, by setting the linkType parameter to linkset, the resolver SHALL NOT redirect the query and SHALL return the linkset. ';
-  ltLinksetNoRedirect.msg =
-    'Setting linkType to linkset resulted in a redirect or an error';
-  recordResult(ltLinksetNoRedirect);
-
-  ltLinksetNoRedirect.url =
-    testUri +
-    '?test=getAllHeaders&testVal=' +
-    encodeURIComponent(stripQueryStringFromURL(dl) + '?linkType=linkset');
-  // console.log(`Testing ${ltLinksetNoRedirect.url}`)
-  ltLinksetNoRedirect.process = async (data) => {
-    // console.log(`What came back was ${data.result['httpCode']}`);
-    if (data.result['httpCode'] === 200) {
-      ltLinksetNoRedirect.status = 'pass';
-      ltLinksetNoRedirect.msg = 'No redirect with linkType set to linkset';
-      linksetGetMethods['linkset'] = true; // Record that this method worked for when we want to get the linkset
-      recordResult(ltLinksetNoRedirect);
-    } else if (
-      firstRun &&
-      (data.result['location'] !== undefined ||
-        data.result['Location'] !== undefined)
-    ) {
-      // This happens if we ask for a DL URI with a qualifier (like a batch or serial)
-      // And resolver redirects to the 'root' identifier. This is perfectly OK but we
-      // need to make a second query to get the linkset
-      // Start by looking for a location header (case-insensitive)
-
-      let ltLocation =
-        data.result['location'] !== undefined
-          ? data.result['location']
-          : data.result['Location'];
-      if (dl.indexOf(stripQueryStringFromURL(ltLocation)) === 0) {
-        // Meaning that the URI we were redirected to is a less-granular version
-        // of our original dl
-        // console.log(`Here with a location of ${ltLocation}`)
-        linksetGetMethods['location'] = ltLocation;
-        await runTest(linkTypeIslinkset(ltLocation, false));
-        // Calls itself with the new (shorter) URL.
-        // Setting the firstRun flag to false stops it calling itself more than once
-      }
-    } else {
-      // OK so we haven't got a good result from linkType=linkset
-      // Let's try the deprecated linkType=all
-      await runTest(linkTypeIsAll(dl, true));
-    }
-  };
-  return ltLinksetNoRedirect;
-};
-
-const linkTypeIsAll = (dl, firstRun) => {
-  // If linkType=linkset didn't work, we'll try this
-  // The code is almost the same as when checking linkType=linkset
-  let ltAllNoRedirect = Object.create(resultProps);
-  ltAllNoRedirect.id = 'ltAllNoRedirect';
-  ltAllNoRedirect.test =
-    'On receiving a request for the linkset, by setting the linkType paramter to all, the resolver SHALL NOT redirect the query and SHALL return the linkset. ';
-  ltAllNoRedirect.msg =
-    'Setting linkType to all resulted in a redirect or an error';
-  recordResult(ltAllNoRedirect);
-
-  ltAllNoRedirect.url =
-    testUri +
-    '?test=getAllHeaders&testVal=' +
-    encodeURIComponent(stripQueryStringFromURL(dl) + '?linkType=all');
-  // console.log(`Testing ${ltAllNoRedirect.url}`)
-  ltAllNoRedirect.process = async (data) => {
-    if (data.result['httpCode'] === 200) {
-      ltAllNoRedirect.status = 'warn';
-      ltAllNoRedirect.msg =
-        'Using linkType set to all was deprecated in GS1-Conformant resolver 1.0.0 (Feb 2024) in favour of linkType=linkset';
-      linksetGetMethods['all'] = true; // Record that this method worked for when we want to get the linkset
-      recordResult(ltAllNoRedirect);
-    } else if (
-      firstRun &&
-      (data.result['location'] !== undefined ||
-        data.result['Location'] !== undefined)
-    ) {
-      let ltLocation =
-        data.result['location'] !== undefined
-          ? data.result['location']
-          : data.result['Location'];
-      if (dl.indexOf(stripQueryStringFromURL(ltLocation)) === 0) {
-        // Meaning that the URI we were redirected to is a less-granular version
-        // of our original dl
-        // console.log(`Here with a location of ${ltLocation}`)
-        linksetGetMethods['location'] = ltLocation;
-        await runTest(ltAllNoRedirect(ltLocation, false));
-        // Calls itself with the new (shorter) URL.
-        // Setting the firstRun flag to false stops it calling itself more than once
-      }
-    }
-  };
-  return ltAllNoRedirect;
-};
-
-const ltWithAcceptHeader = (dl, firstRun) => {
-  // This version of the same code used to try and get the linkset by setting
-  // the HTTP Accept header to application/linkset+json
-
-  let ltAcceptHeader = Object.create(resultProps);
-  ltAcceptHeader.id = 'ltAcceptHeader';
-  ltAcceptHeader.test =
-    'On receiving a request for the linkset, by setting the HTTP Accept Header to application/linkset+json, the resolver SHALL NOT redirect the query and SHALL return the linkset. ';
-  ltAcceptHeader.msg =
-    'Setting HTTP Accept header to application/linkset+json resulted in a redirect or an error';
-  recordResult(ltAcceptHeader);
-  // Note use of getLinksetHeaders in the query to the tester.php service
-  ltAcceptHeader.url =
-    testUri +
-    '?test=getLinksetHeaders&testVal=' +
-    encodeURIComponent(stripQueryStringFromURL(dl) + '?linkType=all');
-  // console.log(`Testing ${ltAcceptHeader.url}`)
-  ltAcceptHeader.process = async (data) => {
-    if (data.result['httpCode'] === 200) {
-      ltAcceptHeader.status = 'pass';
-      ltAcceptHeader.msg =
-        'Setting HTTP Accept header to application/linkset+json did not result in a redirect';
-      linksetGetMethods['accept'] = true; // Record that this method worked for when we want to get the linkset
-      recordResult(ltAcceptHeader);
-    } else if (
-      firstRun &&
-      (data.result['location'] !== undefined ||
-        data.result['Location'] !== undefined)
-    ) {
-      let ltLocation =
-        data.result['location'] !== undefined
-          ? data.result['location']
-          : data.result['Location'];
-      if (dl.indexOf(stripQueryStringFromURL(ltLocation)) === 0) {
-        // Meaning that the URI we were redirected to is a less-granular version
-        // of our original dl
-        linksetGetMethods['location'] = ltLocation;
-        await runTest(ltWithAcceptHeader(ltLocation, false));
-        // Calls itself with the new (shorter) URL.
-        // Setting the firstRun flag to false stops it calling itself more than once
-      }
-    }
-  };
-  return ltAcceptHeader;
-};
-
-const fetchAndValidateTheLinkset = (dl) => {
-  let validLinkset = Object.create(resultProps);
-  validLinkset.id = 'validLinkset';
-  validLinkset.test =
-    'Linkset must be valid according to published JSON schema';
-  validLinkset.msg = 'Linkset does not validate';
-  recordResult(validLinkset);
-
-  // We'll also check the HTTP headers we got with the linkset
-  let linksetJsonldCheck = Object.create(resultProps);
-  linksetJsonldCheck.id = 'linksetJsonldCheck';
-  linksetJsonldCheck.test =
-    'When requesting the linkset the HTTP response headers SHOULD include a Link header pointing to a JSON-LD context file';
-  linksetJsonldCheck.msg =
-    'Link to JSON-LD context file not detected when requesting linkset';
-  linksetJsonldCheck.status = 'warn';
-  recordResult(linksetJsonldCheck);
-
-  let declaredContentType = Object.create(resultProps);
-  declaredContentType.id = 'declaredContentType';
-  declaredContentType.test =
-    'If the HTTP Accept header is application/linkset+json, the resolver SHALL return the linkset serialised as JSON as defined by RFC9264';
-  declaredContentType.msg =
-    'Linkset retrived from the resolver does not declare the content type as application/linkset+json';
-  recordResult(declaredContentType);
-
-  // We'll use the global linksetGetMethods object to help decide which
-  // way to get the linkset
-  let u =
-    linksetGetMethods['location'] === undefined
-      ? stripQueryStringFromURL(dl)
-      : linksetGetMethods['location'];
-  // We'll use the linkType parameter only if the accept header method doesn't work
-  if (!linksetGetMethods['accept']) {
-    if (linksetGetMethods['linkset']) {
-      u += '?linkType=linkset';
-    } else if (linksetGetMethods['all']) {
-      u += '?linkType=all';
-    }
-  }
-  // Note the use of the getLinkset funtion in tester.php which uses GET not the usual HEAD
-  // It sets the Accept Header to application/linkset+json for all requests, whether
-  // or not we're appending a linkType parameter.
-  // Bottom line, if there is a linkset, one way or another, this should fetch it
-  validLinkset.url =
-    testUri + '?test=getLinkset&testVal=' + encodeURIComponent(u);
-  //console.log(`Testing ${validLinkset.url}`)
-  validLinkset.process = async (data) => {
-    let resultObject = JSON.parse(data.result);
-    //console.log(`HTTP Code here is ${resultObject['httpCode']}`)
-    // Need to urldecode the returned linkset
-    const decodedString = decodeURIComponent(resultObject.responseBody);
-    // console.log(`decoded string is ${decodedString}`)
-    let tempObject;
-
-    try {
-      tempObject = JSON.parse(decodedString);
-    } catch (e) {
-      console.log('sendOutput() Error: ' + e.message);
-    }
-
-    // Development linkset is inserted here. In prod mode, we use the received linkset
-    // linksetObject = modelLinkset;
-    // console.log(`using the model linkset`);
-
-    const schemaTestResult = await doesJSONSchemaPass(
-      tempObject,
-      gs1LinksetSchema
-    );
-    if (schemaTestResult.testResult) {
-      validLinkset.msg = 'Linkset validates against the published schema';
-      validLinkset.status = 'pass';
-      recordResult(validLinkset);
-      await linksetTests(dl, tempObject);
-    }
-
-    if (tempObject !== undefined) {
-      // work around not quite right linkset implementation in current prod resolver
-      // Seems to have been fixed 2025-06-02 so this hack not used. Good.
-      const linksetObject = tempFixLinkset(tempObject);
-      const schemaTestResult = await doesJSONSchemaPass(
-        linksetObject,
-        gs1LinksetSchema
-      );
-      if (schemaTestResult.testResult) {
-        validLinkset.msg = 'Linkset validates against the published schema';
-        validLinkset.status = 'pass';
-        recordResult(validLinkset);
-        await linksetTests(dl, linksetObject);
-      }
-    }
-    // Now back to the headers
-    // First we'll loo for the link to the JSON-LD context file
-    let linkHeader = '';
-    if (resultObject.headers.link) {
-      linkHeader = resultObject.headers.link;
-    } else if (resultObject.headers.link) {
-      linkHeader = resultObject.headers.link;
-    }
-    let allLinks = linkHeader.split(',');
-    for (i in allLinks) {
-      // console.log(`looking at ${allLinks[i]}`)
-      if (
-        allLinks[i].indexOf('rel="http://www.w3.org/ns/json-ld#context"') !==
-          -1 &&
-        allLinks[i].indexOf('type="application/ld+json"') !== -1
-      ) {
-        linksetJsonldCheck.msg = 'Link to JSON-LD context file found';
-        linksetJsonldCheck.status = 'pass';
-        recordResult(linksetJsonldCheck);
-        break;
-      }
-    }
-    // Now we can see whether the declared content type is correct
-    let contentType = '';
-    if (resultObject.headers['content-type']) {
-      contentType = resultObject.headers['content-type'];
-    } else if (resultObject.headers['Content-Type']) {
-      contentType = resultObject.headers['Content-Type'];
-    }
-    if (contentType === 'application/linkset+json') {
-      declaredContentType.status = 'pass';
-      declaredContentType.msg =
-        'Content type for the linkset retrieved from the resolver matches the requested application/linkset+json';
-    } else if (contentType === 'application/json') {
-      declaredContentType.status = 'warn';
-      declaredContentType.msg =
-        'Content type for the linkset retrieved from the resolver was application/json but the more specific application/linkset+json was requested';
-    }
-    recordResult(declaredContentType);
-  };
-  return validLinkset;
-};
-
-// This is a temporary hack around the weird behavious of id.gs1.org where
-// linksets are badly formed. Should be removed when new version of GO resolver
-// goes live in September 2025
-// Now seems unnecessary 2025-06-02
-const tempFixLinkset = (linksetObject) => {
-  if (Array.isArray(linksetObject.linkset)) {
-    return linksetObject;
-  } else {
-    let modifiedObject = { linkset: [linksetObject.linkset] };
-    //console.log(`modified version is ${JSON.stringify(modifiedObject)}`);
-    let hackedLinkset = Object.create(resultProps);
-    hackedLinkset.id = 'hackedLinkset';
-    hackedLinkset.test =
-      'Linkset must be valid according to published JSON schema';
-    hackedLinkset.msg =
-      'This is a temporary hack - the linkset is *not* valid as the value of linkset itself must be an array of objects';
-    recordResult(hackedLinkset);
-    console.log(
-      `Linkset under test has been modified (to make it conformant) by putting the linkset in an array`
-    );
-    return modifiedObject;
-  }
-};
-
-const doesJSONSchemaPass = async (data, schemaUrl) => {
-  try {
-    // Initialize Ajv
-    // Set the strict flag to false as it currently doesn't seem to support format type of uri
-    // Need to investigate
-    const ajv = new window.ajv7({ strict: false });
-
-    // Fetch schema from URL
-    const fetchSchema = async () => {
-      try {
-        const response = await fetch(schemaUrl, {
-          method: 'GET',
-          headers: {
-            Accept: 'application/json',
-          },
-        });
-
-        if (!response.ok) {
-          throw new Error(`Failed to fetch schema: ${response.statusText}`);
-        }
-
-        const schema = await response.json();
-
-        // If the schema specifies Draft-04, replace it with Draft-07
-        // Need to update the actual schema
-        if (schema.$schema === 'http://json-schema.org/draft-04/schema#') {
-          schema.$schema = 'http://json-schema.org/draft-07/schema#';
-        }
-
-        return schema;
-      } catch (error) {
-        console.error('Error fetching schema:', error.message);
-        throw new Error('Unable to fetch schema');
-      }
-    };
-
-    // Fetch the schema
-    let schema;
-    try {
-      schema = await fetchSchema();
-    } catch (error) {
-      return {
-        testResult: false,
-        errors: [error.message],
-      };
-    }
-
-    // Compile schema and validate data
-    try {
-      const validate = ajv.compile(schema);
-      const valid = validate(data);
-
-      if (valid) {
-        return { testResult: true }; // Success
-      } else {
-        return {
-          testResult: false,
-          errors: validate.errors.map(
-            (err) => `${err.instancePath} ${err.message}`
-          ),
-        };
-      }
-    } catch (error) {
-      console.error('Validation error:', error.message);
-      return {
-        testResult: false,
-        errors: [error.message],
-      };
-    }
-  } catch (error) {
-    console.error('Unexpected error:', error.message);
-    return {
-      testResult: false,
-      errors: ['Unexpected error occurred during schema validation'],
-    };
-  }
-};
-
-const testQuery = (dl) => {
-  let qsPassedOn = Object.create(resultProps);
-  qsPassedOn.id = 'qsPassedOn';
-  qsPassedOn.test =
-    'By default, SHALL pass on all key=value pairs in the query string of the request URI (if present) when redirecting';
-  qsPassedOn.msg =
-    'Query string not passed on. If the query string is deliberately suppressed for this Digital Link URI, test another one where the default behaviour of passing on the query string applies';
-  qsPassedOn.status = 'warn';
-  recordResult(qsPassedOn);
-  let u = stripQueryStringFromURL(dl);
-  let query = 'foo=bar';
-  qsPassedOn.url =
-    testUri +
-    '?test=getAllHeaders&testVal=' +
-    encodeURIComponent(u + '?' + query);
-  //console.log(`We're testing the query string in this ${testUri + '?test=getAllHeaders&testVal=' + encodeURIComponent(u + '?' + query)}`);
-  qsPassedOn.process = async (data) => {
-    let redirect;
-    if (data.result.location) {
-      redirect = data.result.location;
-    } else if (data.result.Location) {
-      redirect = data.result.Location;
-    }
-    if (redirect) {
-      // There is a redirection
-      if (redirect.indexOf(query) > -1) {
-        // Our query is being passed on
-        qsPassedOn.status = 'pass';
-        qsPassedOn.msg = 'Query passed on when redirecting';
-      }
-    } else {
-      qsPassedOn.msg =
-        'GS1 Digital Link URI not redirected so cannot test this feature';
-    }
-    recordResult(qsPassedOn);
-  };
-  return qsPassedOn;
-};
-
-const linksetTests = async (dl, linksetObject) => {
-  // This routine is called only after we know we have a valid linkset
-  // That means we can make some assumptions about its structure
-  // However, we don't know how many relavant anchors it has. There should be at least
-  // one against which we can match our incoming dl, but we may have to
-  // chop the end off the dl to find the match
-
-  // First a bit of hosekeeping.
-  // We need to handle the 3 namespaces that can be used for GS1 link types
-  // Reduce them all to 'gs1:'
-  // We'll stringify the linkset, do a search and replace, and re-parse it.
-  let linksetObjectAsString = JSON.stringify(linksetObject);
-  const re = /https:\/\/((ref\.)|(www\.))?gs1\.org\/voc\//gi;
-  linksetObjectAsString = linksetObjectAsString.replace(re, 'gs1:');
-  //console.log(linksetObjectString);
-  const workingLinksetObject = JSON.parse(linksetObjectAsString);
-
-  // Now we will work through the linksetObject looking for anchors that
-  // match our dl and then, if there are qualifier paths, remove those and look
-  // again, i.e. walk up the tree.
-  // So if the path is /01/{gtin}/10/{serial} we'll look for an anchor that matches
-  // that, then chop off the last two segments and look for a match for just /01/{gtin}
-  // As we do this, we can build up a picture of the link types for which we have links
-  // If the same link type appears at multiple levels only the most granular level applies
-
-  // We're most concerned with path elements so let's get those somewhere we can
-  // refer to them and manage them if we want to
-  const UriElements = dl.match(RabinRegEx); // (see near the top of this code library)
-  let pathElements = UriElements[10].split('/');
-  let currentDL = stripQueryStringFromURL(dl); // As usual, query strings are irrelevant
-
-  // We need to keep track of the link types found and processed
-  let linkTypesAvailable = [];
-
-  // We know there should be a default link so we can set that object up outside
-  // of the loop
-
-  let defaultLinkExists = Object.create(resultProps);
-  defaultLinkExists.id = 'defaultLinkExists';
-  defaultLinkExists.test =
-    'For each identified entity there SHALL be exactly one default link, the list of link types for which SHALL include gs1:defaultLink. This default is defined without any of the optional attributes, that is, it SHALL include a title, but SHALL NOT include other attributes.';
-  defaultLinkExists.msg = 'No default link found';
-  recordResult(defaultLinkExists);
-
-  while (pathElements.length > 2) {
-    // This outer loop works back from the right hand side of the dl
-    for (i in workingLinksetObject.linkset) {
-      // The working linkset is an array with one anchor per object
-      // It's that anchor we want to match against our currentDL
-      if (workingLinksetObject.linkset[i].anchor === currentDL) {
-        // We have a match for this DL
-        // Now we need to see what link types there are for this anchor
-        // And then, for each link type, process the link(s) found
-
-        for (j in workingLinksetObject.linkset[i]) {
-          // So we're working our way through the array that is the value of the
-          // `linkset` property
-          if (j !== 'anchor' && j !== 'itemDescription') {
-            // In a valid linset, if it's not the anchor or the itemDescription,
-            // it must be a link type
-            const currentLinkType = j;
-            // The same link type may be used higher up the hierarchy but we
-            // must only check it at the most granular level for which there
-            // is a match. Therefore, we only process this link type if we haven't
-            // seen it already. To do that, we need to keep a note of what we
-            // have done
-            if (!linkTypesAvailable.includes(currentLinkType)) {
-              linkTypesAvailable.push(currentLinkType);
-
-              // We'll start by looking for a defaultLink at this level
-              if (currentLinkType === 'gs1:defaultLink') {
-                defaultLinkExists.msg = 'Default link found';
-                defaultLinkExists.status = 'pass';
-                recordResult(defaultLinkExists);
-                // If we have a defaultLink, there should only be one link object
-                // Let's check and record that result
-                let singleDefaulLink = Object.create(resultProps);
-                singleDefaulLink.id = 'singleDefaulLink';
-                singleDefaulLink.test =
-                  'For each identified entity there SHALL be exactly one default link, the list of link types for which SHALL include gs1:defaultLink. This default is defined without any of the optional attributes, that is, it SHALL include a title, but SHALL NOT include other attributes.';
-                if (
-                  workingLinksetObject.linkset[i][currentLinkType].length === 1
-                ) {
-                  singleDefaulLink.status = 'pass';
-                  singleDefaulLink.msg = 'Single default link found';
-                } else if (
-                  workingLinksetObject.linkset[i][currentLinkType].length !== 1
-                ) {
-                  singleDefaulLink.msg =
-                    'Multiple (or zero) default links found';
-                }
-                recordResult(singleDefaulLink);
-                // We should be redirected to the default's href with a simple query
-                // console.log(`Is our default link ${workingLinksetObject.linkset[i]['gs1:defaultLink'][0].href} ?`)
-                if (
-                  singleDefaulLink.status === 'pass' &&
-                  workingLinksetObject.linkset[i]['gs1:defaultLink'][0].href
-                ) {
-                  await runTest(
-                    testDefaultLink(
-                      dl,
-                      workingLinksetObject.linkset[i]['gs1:defaultLink'][0].href
-                    )
-                  );
-                }
-              } else if (
-                workingLinksetObject.linkset[i][currentLinkType].length === 1
-              ) {
-                // There is a single possible redirect for this link type, nothing else matters
-                // at this point so we're looking for a simple redirect
-                // If the current link type is gs1:defaultLinkMulti that deserves a warning
-                // as it makes no sense to have a single "defaultMulti"
-                if (currentLinkType === 'gs1:defaultLinkMulti') {
-                  let singleMultiLink = Object.create(resultProps);
-                  singleMultiLink.id = 'singleMultiLink';
-                  singleMultiLink.test = `Support for gs1:defaultLinkMulti is optional but, if supported, it allows a resolver to take note of, for example, a user‚Äôs language preferences to determine which of several available redirects are followed.`;
-                  singleMultiLink.status = 'warn';
-                  singleMultiLink.msg = `A single link is defined for gs1:defaultLinkMulti. This is inconsistent with its intended use`;
-                  recordResult(singleMultiLink);
-                } else {
-                  // console.log(`Link type of ${currentLinkType} should redirect to ${workingLinksetObject.linkset[i][currentLinkType][0].href}`)
-                  await runTest(
-                    testSingleLinkObject(
-                      dl,
-                      currentLinkType,
-                      workingLinksetObject.linkset[i][currentLinkType][0].href
-                    )
-                  );
-                }
-              } else if (
-                workingLinksetObject.linkset[i][currentLinkType].length > 1
-              ) {
-                // Here we have more than one link object for a given link type
-                // We take into consideration the type and hreflang attributes
-                // The context attribute is optional and its value space undefined in
-                // the standard, therefore we cannot use it formally in these tests
-                // However, we do need to look at it because a resolver may use it in
-                // any way it wants and so we can't expect a 300 response for link objects
-                // where the context is different, even though we're not testing it directly
-
-                // Set up an array for this link type that stores the array position
-                // of link objects with identical attributes
-                // These should return an HTTP 300 response
-                let threehundredLinks = [];
-
-                for (linkObject in workingLinksetObject.linkset[i][
-                  currentLinkType
-                ]) {
-                  // Start with the first one and put its attributes in a mini object
-                  // f is the one for which we're looking for any exact matches (m)
-
-                  for (
-                    let f = 0;
-                    f < workingLinksetObject.linkset[i][currentLinkType].length;
-                    f++
-                  ) {
-                    if (!threehundredLinks.includes(f)) {
-                      // If we haven't already got this in our list of 300s
-                      const lo1 = {};
-                      if (
-                        workingLinksetObject.linkset[i][currentLinkType][f].type
-                      ) {
-                        lo1.type =
-                          workingLinksetObject.linkset[i][currentLinkType][
-                            f
-                          ].type;
-                      }
-                      if (
-                        workingLinksetObject.linkset[i][currentLinkType][f]
-                          .hreflang
-                      ) {
-                        lo1.hreflang =
-                          workingLinksetObject.linkset[i][currentLinkType][
-                            f
-                          ].hreflang;
-                      }
-                      if (
-                        workingLinksetObject.linkset[i][currentLinkType][f]
-                          .context
-                      ) {
-                        lo1.context =
-                          workingLinksetObject.linkset[i][currentLinkType][
-                            f
-                          ].context;
-                      }
-
-                      // Now we look through the other link objects, noting exact matches
-                      // If we find one, we record the two under test in the threehundredLinks array
-
-                      for (
-                        let m = 0;
-                        m <
-                        workingLinksetObject.linkset[i][currentLinkType].length;
-                        m++
-                      ) {
-                        if (f !== m && !threehundredLinks.includes(m)) {
-                          const lo2 = {};
-                          if (
-                            workingLinksetObject.linkset[i][currentLinkType][m]
-                              .type
-                          ) {
-                            lo2.type =
-                              workingLinksetObject.linkset[i][currentLinkType][
-                                m
-                              ].type;
-                          }
-                          if (
-                            workingLinksetObject.linkset[i][currentLinkType][m]
-                              .hreflang
-                          ) {
-                            lo2.hreflang =
-                              workingLinksetObject.linkset[i][currentLinkType][
-                                m
-                              ].hreflang;
-                          }
-                          if (
-                            workingLinksetObject.linkset[i][currentLinkType][m]
-                              .context
-                          ) {
-                            lo2.context =
-                              workingLinksetObject.linkset[i][currentLinkType][
-                                m
-                              ].context;
-                          }
-                          if (identicalAttributes(lo1, lo2)) {
-                            if (!threehundredLinks.includes(f)) {
-                              threehundredLinks.push(m);
-                            }
-                            if (!threehundredLinks.includes(m)) {
-                              threehundredLinks.push(m);
-                            }
-                          }
-                        }
-                      }
-                    }
-                  }
-                }
-                // console.log(`For ${currentLinkType} Check any of these to get a 300 ${threehundredLinks}`)
-                // console.log(`For ${currentLinkType} Check all that are not in ${threehundredLinks} individually`)
-                await testMultipleLinks(
-                  dl,
-                  currentLinkType,
-                  workingLinksetObject.linkset[i][currentLinkType],
-                  threehundredLinks
-                );
-              }
-            }
-          }
-        }
-      }
-    }
-    // Remove last two segments and go round again
-    currentDL = currentDL.substring(0, currentDL.lastIndexOf('/'));
-    currentDL = currentDL.substring(0, currentDL.lastIndexOf('/'));
-    pathElements.pop();
-    pathElements.pop();
-  }
-  // console.log(`I have my link Types which are ${linkTypesAvailable}`)
-  // Before we finish this marathon, let's just check that all the link types
-  // that we have found are in the GS1 set
-  // checkGS1LinkTypes(linkTypesAvailable);
-  let linkTypesDefined = Object.create(resultProps);
-  linkTypesDefined.id = 'linkTypesDefined';
-  linkTypesDefined.test =
-    'Link types SHOULD be given as a URI defined in the GS1 Web vocabulary';
-  linkTypesDefined.status = 'warn';
-  linkTypesDefined.msg =
-    'Link type detected that claims to be defined in the GS1 Web Vocabulary but is not';
-  recordResult(linkTypesDefined);
-
-  // Fetch the current list of link types
-  const fetchRatifiedLinkTypes = async () => {
-    try {
-      const response = await fetch('https://ref.gs1.org/voc/data/linktypes', {
-        method: 'GET',
-        headers: {
-          Accept: 'application/json',
-        },
-      });
-
-      if (!response.ok) {
-        throw new Error(
-          `Failed to fetch ratified link type list: ${response.statusText}`
-        );
-      }
-
-      const list = await response.json();
-
-      return list;
-    } catch (error) {
-      console.error('Error fetching list:', error.message);
-      throw new Error('Unable to fetch list');
-    }
-  };
-
-  // Fetch the list
-  let list;
-  try {
-    list = await fetchRatifiedLinkTypes();
-  } catch (error) {
-    return {
-      testResult: false,
-      errors: [error.message],
-    };
-  }
-  // Set overall result to pass
-  linkTypesDefined.status = 'pass';
-  linkTypesDefined.msg =
-    'All GS1 link types found are in the current ratified set';
-
-  // Now look for any that aren't in the list and reset back to warn if needs be
-  for (lt in linkTypesAvailable) {
-    if (linkTypesAvailable[lt].indexOf('gs1:') === 0) {
-      // We're only looking at GS1 link types
-      linkTypeName = linkTypesAvailable[lt].substring(4);
-      if (list[linkTypeName] === undefined) {
-        linkTypesDefined.status = 'warn';
-        linkTypesDefined.msg = `gs1:${linkTypeName} is not a ratified GS1 link type`;
-      }
-    }
-  }
-  recordResult(linkTypesDefined);
-};
-
-const testDefaultLink = (dl, targetURL) => {
-  let defaultTarget = Object.create(resultProps);
-  defaultTarget.id = 'defaultTarget';
-  defaultTarget.test =
-    'One of the links SHALL be recognised by the resolver as the default and the resolver SHALL redirect to that URL unless there is information supplied within the query to the contrary.';
-  defaultTarget.msg = 'Resolver does not redirect to the default as expected';
-  recordResult(defaultTarget);
-  let u = stripQueryStringFromURL(dl);
-  defaultTarget.url =
-    testUri + '?test=getAllHeaders&testVal=' + encodeURIComponent(u);
-  defaultTarget.process = async (data) => {
-    let targetLocation = data.result.Location;
-    if (!targetLocation) {
-      targetLocation = data.result.location;
-    }
-    if (targetURL === targetLocation) {
-      defaultTarget.status = 'pass';
-      defaultTarget.msg = 'Resolver correctly redirects to default';
-      recordResult(defaultTarget);
-    }
-  };
-  return defaultTarget;
-};
-
-const testSingleLinkObject = (dl, linkType, targetURL) => {
-  let loObject = Object.create(resultProps);
-  loObject.id = 'loFor' + linkType.replace(':', '');
-  loObject.test =
-    'If the requested type of link is available, the resolver SHALL redirect to it';
-  loObject.msg = `Requesting linkType of ${linkType} did not redirect to ${targetURL}`;
-  loObject.url =
-    testUri +
-    '?test=getAllHeaders&testVal=' +
-    encodeURIComponent(stripQueryStringFromURL(dl) + '?linkType=' + linkType);
-  // console.log(`Single link object fetching ${loObject.url}`)
-  recordResult(loObject);
-  loObject.process = async (data) => {
-    let targetLocation = data.result.Location;
-    if (!targetLocation) {
-      targetLocation = data.result.location;
-    }
-    if (targetLocation.indexOf('?linkType=' + linkType) !== -1) {
-      targetLocation = stripQueryStringFromURL(targetLocation);
-    } else if (targetLocation.indexOf('&linkType=' + linkType) !== -1) {
-      targetLocation = targetLocation.substring(
-        0,
-        targetLocation.indexOf('&linkType=')
-      );
-    }
-    //console.log(`targetURL is ${targetURL} and targetLocation is ${targetLocation}, test is ${(targetURL === targetLocation)}`)
-    if (targetURL === targetLocation) {
-      loObject.status = 'pass';
-      loObject.msg = `Resolver correctly redirects to ${targetURL} for ${linkType} `;
-      recordResult(loObject);
-    }
-  };
-  return loObject;
-};
-
-const testMultipleLinks = async (
-  dl,
-  linkType,
-  arrayOfLinkObjects,
-  threehundredLinks
-) => {
-  let done300 = false;
-  // console.log(`Looking at ${linkType} with its ${arrayOfLinkObjects.length} LOs, noting the 300s ${threehundredLinks}.`)
-  for (lo in arrayOfLinkObjects) {
-    let lang = '';
-    let context = '';
-    let mediaType = '';
-    let loObject = Object.create(resultProps);
-    loObject.id = 'loFor' + linkType.replace(':', '') + lo;
-    if (threehundredLinks.includes(parseInt(lo)) && done300) {
-      // console.log(`Done`)
-      continue;
-    } else if (threehundredLinks.includes(parseInt(lo))) {
-      // console.log(`Our link object ${lo} is in the 300Links array`)
-      // We'll only test one of the links in the threehundredLinks array.
-      // We'll assume that if the resolver returns a 300 for one, it will do so for all
-      loObject.test =
-        'Where it is impossible to determine which of multiple links of the same link type is the best match, the resolver should return the list of the available links of that type with an HTTP response code of 300 Multiple Choices.';
-      loObject.msg = `A request for which the linkset contains multiple links for ${linkType} with the following identical attributes did not return a 300 response: `;
-      loObject.status = 'warn';
-      loObject.process = async (data) => {
-        if (data.result['httpCode'] === 300) {
-          loObject.status = 'pass';
-          loObject.msg =
-            'Resolver returned a 300 result when queried for a link for which it has multiple possible responses.';
-          recordResult(loObject);
-        }
-      };
-      done300 = true;
-    } else {
-      // console.log(`Our link object ${lo} is not in the 300Links array`)
-      // So now we're looking for a redirect, not a 300
-      loObject.test =
-        'If the requested type of link is available, the resolver SHALL redirect to it';
-      loObject.msg = `Requesting linkType of ${linkType} with the following parameters did not redirect to ${arrayOfLinkObjects[lo].href}`;
-      loObject.process = async (data) => {
-        // Need to handle 'location' and 'Location'
-        // Need to handle linkType in the target location query string whether on its own or added to existng query
-        let targetLocation = data.result.Location;
-        if (!targetLocation) {
-          targetLocation = data.result.location;
-        }
-        if (targetLocation.indexOf('?linkType=' + linkType) !== -1) {
-          targetLocation = stripQueryStringFromURL(targetLocation);
-        } else if (targetLocation.indexOf('&linkType=' + linkType) !== -1) {
-          targetLocation = targetLocation.substring(
-            0,
-            targetLocation.indexOf('&linkType=')
-          );
-        }
-        // Now we can do the comparison
-        if (arrayOfLinkObjects[lo].href === targetLocation) {
-          loObject.status = 'pass';
-          loObject.msg = loObject.msg.replace('did not redirect', 'redirected');
-        }
-        recordResult(loObject);
-      };
-    }
-    // In all cases, we need to construct the request with all the relevant parameters
-    let queryString = '?';
-    if (linkType !== 'gs1:defaultLinkMulti') {
-      // Never ask for defaultLinkMulti explicitly
-      queryString = '?linkType=' + linkType;
-    }
-    if (arrayOfLinkObjects[lo].type !== null) {
-      loObject.msg += ` type: ${arrayOfLinkObjects[lo].type}`;
-      if (queryString.length > 1) {
-        // So this isn't the first name=value pair
-        queryString += '&';
-      }
-      queryString += `mediaType=${arrayOfLinkObjects[lo].type}`;
-    }
-    if (Array.isArray(arrayOfLinkObjects[lo].hreflang)) {
-      loObject.msg += ` hreflang: ${arrayOfLinkObjects[lo].hreflang[0]}`;
-      if (queryString.length > 1) {
-        // So this isn't the first name=value pair
-        queryString += '&';
-      }
-      queryString += `lang=${arrayOfLinkObjects[lo].hreflang[0]}`;
-    }
-    if (
-      arrayOfLinkObjects[lo].context !== undefined &&
-      Array.isArray(arrayOfLinkObjects[lo].context)
-    ) {
-      if (queryString.length > 1) {
-        // So this isn't the first name=value pair
-        queryString += '&';
-      }
-      loObject.msg += ` context: ${arrayOfLinkObjects[lo].context[0]}`;
-      queryString += `context=${arrayOfLinkObjects[lo].context[0]}`;
-    } else if (arrayOfLinkObjects[lo].context !== undefined) {
-      // console.log(`here with something else and ${arrayOfLinkObjects[lo].context}`)
-      loObject.msg += ` context: ${arrayOfLinkObjects[lo].context}`;
-      queryString += `context=${arrayOfLinkObjects[lo].context}`;
-    }
-    loObject.url =
-      testUri +
-      '?test=getAllHeaders&testVal=' +
-      encodeURIComponent(stripQueryStringFromURL(dl) + queryString);
-    console.log(`fetching ${loObject.url}`);
-    recordResult(loObject);
-    await runTest(loObject);
-  }
-};
-
-const identicalAttributes = (lo1, lo2) => {
-  let typeMatch = false;
-  let hreflangMatch = false;
-  let contextMatch = false;
-  if (!lo1.type && !lo2.type) {
-    // Both are undefined
-    typeMatch = true;
-  } else if (lo1.type && lo2.type && lo1.type === lo2.type) {
-    // Both are defined and are the same
-    typeMatch = true;
-  } // In any other circumstance, there must be a difference.
-  // Repeat for hreflang
-  if (!lo1.hreflang && !lo2.hreflang) {
-    //Both are undefined
-    hreflangMatch = true;
-  } else if (
-    lo1.hreflang &&
-    lo2.hreflang &&
-    lo1.hreflang.every((val, idx) => val === lo2.hreflang[idx])
-  ) {
-    // Both are defined and the arrays of hreflangs are equivalent
-    hreflangMatch = true;
-  }
-  // Repeat for context, but it could be a string or an array
-  // (could conceiveably be an object but thats getting over-complicated so we'll ignore that possibility)
-  if (!lo1.context && !lo2.context) {
-    //Both are undefined
-    contextMatch = true;
-  } else if (lo1.context && lo2.context) {
-    // This has not been tested as of 2025-05-23
-    if (
-      Array.isArray(lo1.context) &&
-      Array.isArray(lo2.context) &&
-      lo1.context.every((val, idx) => val === lo2.context[idx])
-    ) {
-      contextMatch = true;
-    } else if (lo1.context === lo2.context) {
-      contextMatch = true;
-    }
-  }
-
-  return typeMatch && hreflangMatch && contextMatch;
-};
-
-const testFor404 = (dl) => {
-  // A check that if we ask for a link type for which no link is available
-  // the resolver returns a 404
-
-  let specificLinkTypeNotFound = Object.create(resultProps);
-  specificLinkTypeNotFound.id = 'specificLinkTypeNotFound';
-  specificLinkTypeNotFound.test =
-    'If the requested type of link is not available, the resolver SHALL return a 404 Not Found message.';
-  specificLinkTypeNotFound.msg =
-    'Requesting a specific link type for which no link is available did not result in a 404 Not Found response';
-  specificLinkTypeNotFound.url =
-    testUri +
-    '?test=getAllHeaders&testVal=' +
-    encodeURIComponent(stripQueryStringFromURL(dl) + '?linkType=gs1:nosuchlt');
-  recordResult(specificLinkTypeNotFound);
-
-  specificLinkTypeNotFound.process = async (data) => {
-    if (data.result['httpCode'] === 404) {
-      specificLinkTypeNotFound.status = 'pass';
-      specificLinkTypeNotFound.msg =
-        'Requesting a specific link type for which no link is available resulted in a 404 Not Found response';
-      recordResult(specificLinkTypeNotFound);
-    }
-  };
-  return specificLinkTypeNotFound;
-};
-
-const resultSummary = async () => {
-  // Working with the testArray that was defined as a global variable near the top
-  // Let's start by calculating a simple percentage success
-  let pass = 0;
-  let fail = 0;
-  let rdFileText = '';
-  let invalidLinksetText = '';
-  let criticalFailure = 0;
-  for (i in resultsArray) {
-    if (resultsArray[i].status === 'pass') {
-      pass++;
-    }
-    if (resultsArray[i].status === 'fail') {
-      fail++;
-    }
-    if (resultsArray[i].id === 'rdFile') {
-      if (resultsArray[i].status !== 'pass') {
-        // critical failure as Resolver Description File is essential
-        rdFileText = `GS1-Conformant resolvers MUST be declared at /.well-known/gs1resolver so that they can be distinguished from other services`;
-        criticalFailure++;
-      }
-    }
-    if (
-      resultsArray[i].id === 'validLinkset' ||
-      resultsArray[i].id === 'hackedLinkset'
-    ) {
-      if (resultsArray[i].status !== 'pass') {
-        // Critical failure as linkset must be valid
-        invalidLinksetText = `GS1-Conformant resolvers MUST return a valid linkset, formatted as JSON, as defined in RFC 9264`;
-        criticalFailure++;
-      }
-    }
-  }
-  let pc = (100 * pass) / (pass + fail);
-  let percentPassed = parseFloat(pc).toFixed(0);
-  // let text = `You passed ${percentPassed}% of the tests. `;
-  let text = '';
-  let p = document.createElement('p');
-  p.id = 'overallReport';
-  if (criticalFailure) {
-    p.className = 'fail';
-    if (criticalFailure === 1) {
-      text += `There is at least one critical issue: ${rdFileText}${invalidLinksetText}.`;
-    } else {
-      text += `There are critical issues: ${rdFileText} and ${invalidLinksetText}.`;
-    }
-  } else if (pc === 100) {
-    p.className = 'pass';
-  } else if (pc > 60) {
-    p.className = 'lightWarn';
-  } else {
-    p.className = 'warn';
-  }
-  p.appendChild(document.createTextNode(text));
-  if (text !== '') {
-    document.getElementById(outputElement).prepend(p);
-  }
-};
-
-const rotatingCircle = (showFlag) => {
-  document.getElementById('rotatingcircle').style.visibility = showFlag
-    ? 'visible'
-    : 'hidden';
-};
-
-// ********************* Processing functions ******************************
-
-function stripQueryStringFromURL(url) {
-  try {
-    return url.indexOf('?') > -1 ? url.substring(0, url.indexOf('?')) : url;
-  } catch (e) {
-    console.log(
-      `stripQueryStringFromURL() URL: '${url}',  Error: ${e.message}`
-    );
-    // print stacktrace
-    console.log(e.stack);
-  }
-}
-
-/**
- * runTest() executes the .process() method in the supplied test.
- * @param test
- * @returns {Promise<void>}
- */
-const runTest = async (test) => {
-  // console.log('Here with Fetching ' + test.url + ' for ' + test.id);
-  try {
-    let response = await fetch(test.url, { headers: test.headers });
-    let data = await response.json();
-    await test.process(data);
-    // runTest has called recordResult since day 1 but it's unnecessary and
-    // giving odd results in some cases. Commented out 2025-06-02
-    //recordResult(test);
-  } catch (error) {
-    console.log(
-      `"Error from test id: '${test.id}' on '${test.url}' has error: ${error}`
-    );
-    // print stacktrace
-    console.log(error.stack);
-  }
-  return test.status;
-};
-
-// ************************** Output functions *****************************
-
-function getDL() {
-  let dl = document.getElementById('testResults');
-  if (!dl) {
-    dl = document.createElement('dl');
-    dl.id = 'testResults';
-    document.getElementById(outputElement).appendChild(dl);
-  }
-  return dl;
-}
-
-function clearGrid() {
-  let e = document.getElementById('overallReport');
-  if (e) {
-    e.remove();
-  }
-  e = document.getElementById('resultsGrid');
-  if (e) {
-    e.innerHTML = '';
-  }
-  e = document.getElementById('testResults');
-  if (e) {
-    e.innerHTML = '';
-  }
-  return true;
-}
-
-function getGrid() {
-  let p = document.getElementById('resultsGrid');
-  if (!p) {
-    p = document.createElement('p');
-    p.id = 'resultsGrid';
-    document.getElementById(outputElement).appendChild(p);
-  }
-  return p;
-}
-
-function recordResult(result) {
-  try {
-    // See if result is already in the array
-    let i = 0;
-    while (i < resultsArray.length && resultsArray[i].id !== result.id) {
-      i++;
-    }
-    // i now either points to the existing record or the next available index, either way we can now push the result
-    // into the array
-    resultsArray[i] = result;
-    sendOutput(result);
-    return 1;
-  } catch (e) {
-    console.log('recordResult() Error: ' + e.message);
-    return 0;
-  }
-}
-
-function sendOutput(o) {
-  // We begin by creating or updating entries in the DL list
-  // If the dt/dd pair exist we need to update them, otherwise we need to create them
-  // So begin by testing for existence
-
-  try {
-    let dd = document.getElementById(o.id + 'dd');
-    if (dd) {
-      // It exists
-      dd.innerHTML = o.msg;
-      dd.className = o.status;
-    } else {
-      // It doesn't exist so we need to create everything
-      let dt = document.createElement('dt');
-      dt.id = o.id;
-      let t = document.createTextNode(o.test);
-      dt.appendChild(t);
-      dd = document.createElement('dd');
-      dd.id = o.id + 'dd';
-      dd.className = o.status;
-      t = document.createTextNode(o.msg);
-      dd.appendChild(t);
-
-      let dl = getDL();
-      dl.appendChild(dt);
-      dl.appendChild(dd);
-    }
-
-    // Now we want to do the same for the grid
-    let grid = getGrid();
-    // Does the grid square exist?
-    let a = document.getElementById(o.id + 'a');
-    if (a) {
-      // It exists - this will just be a status change
-      a.className = o.status;
-    } else {
-      // It doesn't exist and needs to be created
-      let sq = document.createElement('a');
-      sq.id = o.id + 'a';
-      sq.href = '#' + o.id;
-      sq.className = o.status;
-      sq.title = o.test;
-      grid.appendChild(sq);
-    }
-  } catch (e) {
-    console.log('sendOutput() Error: ' + e.message);
-  }
-}
+src/GS1DigitalLinkResolverTestSuite.js 90ms
--- src/interpretGS1Scan.js	2025-09-16 09:24:42
+++ temp_formatted.js	2025-09-16 20:23:46
@@ -37,12 +37,21 @@
 
 async function interpretScan(scan) {
   let gtinRE = /^(\d{8})$|^(\d{12,14})$/;
-  let e, gs1DigitalLinkURI, gs1ElementStrings, gs1Array, primaryKey, AIstringBrackets, AIstringFNC1, errmsg, gs1dlt;
+  let e,
+    gs1DigitalLinkURI,
+    gs1ElementStrings,
+    gs1Array,
+    primaryKey,
+    AIstringBrackets,
+    AIstringFNC1,
+    errmsg,
+    gs1dlt;
   let dlOrderedAIlist = [];
   let dateAIs = ['11', '12', '13', '15', '17'];
   let doNotEscape = false;
 
-  if (e = scan.match(gtinRE)) {  // So we just have a GTIN (from an EAN/UPC probably)
+  if ((e = scan.match(gtinRE))) {
+    // So we just have a GTIN (from an EAN/UPC probably)
     scan = '(01)' + scan;
     doNotEscape = true;
   } else if (scan.indexOf(String.fromCharCode(29)) == 0) {
@@ -56,54 +65,74 @@
     gs1dlt = new GS1DigitalLinkToolkit();
     if (plausibleDL.any) {
       if (!plausibleDL.uncompressedWithAlphas) {
-        scan = gs1dlt.decompressGS1DigitalLink(scan,false,'https://id.gs1.org');  // Decompress if it's likely to be compressed
+        scan = gs1dlt.decompressGS1DigitalLink(
+          scan,
+          false,
+          'https://id.gs1.org'
+        ); // Decompress if it's likely to be compressed
       }
       // If we're here, the input must have been a DL URI and scan must be decompressed
       try {
-      	gs1ElementStrings = gs1dlt.gs1digitalLinkToGS1elementStrings(scan, true);
+        gs1ElementStrings = gs1dlt.gs1digitalLinkToGS1elementStrings(
+          scan,
+          true
+        );
         gs1DigitalLinkURI = scan;
-      } catch(err) {
-      	console.log(err);
+      } catch (err) {
+        console.log(err);
         errmsg = err;
       }
-    } else {  // Hopefully scan is an element string then, which we can convert to a DL URI, remembering to escape reserved characters first
+    } else {
+      // Hopefully scan is an element string then, which we can convert to a DL URI, remembering to escape reserved characters first
       try {
-      	gs1DigitalLinkURI = doNotEscape ? gs1dlt.gs1ElementStringsToGS1DigitalLink(scan, false, 'https://id.gs1.org') : gs1dlt.gs1ElementStringsToGS1DigitalLink(escapeReservedCharacters(scan), false, 'https://id.gs1.org');
-      } catch(err) {
-     	  console.log(err);
+        gs1DigitalLinkURI = doNotEscape
+          ? gs1dlt.gs1ElementStringsToGS1DigitalLink(
+              scan,
+              false,
+              'https://id.gs1.org'
+            )
+          : gs1dlt.gs1ElementStringsToGS1DigitalLink(
+              escapeReservedCharacters(scan),
+              false,
+              'https://id.gs1.org'
+            );
+      } catch (err) {
+        console.log(err);
         errmsg = err;
       }
     }
     //    console.log('We have a DL of ' + gs1DigitalLinkURI);
-  } catch(err) {
+  } catch (err) {
     console.log(err);
     errmsg = err;
   }
 
   // Whatever the input, we have a DL or an error. If an error, the value of gs1DigitalLinkURI is undefined
   if (gs1DigitalLinkURI == undefined) {
-    return {'errmsg' : errmsg}
+    return { errmsg: errmsg };
   } else {
     try {
       gs1Array = gs1dlt.extractFromGS1digitalLink(gs1DigitalLinkURI);
-    } catch(err) {
-    	console.log(err);
-      return {'errmsg' : err}  // Quit here if we have an error
+    } catch (err) {
+      console.log(err);
+      return { errmsg: err }; // Quit here if we have an error
     }
 
     // Want to find the primary identifier
-    // We'll use the aitable 
-    let done = [];  // Use this to keep track of what we've done
+    // We'll use the aitable
+    let done = []; // Use this to keep track of what we've done
     for (i in gs1Array.GS1) {
-      if (gs1dlt.aitable.find(x => x.ai === i).type === 'I') {
+      if (gs1dlt.aitable.find((x) => x.ai === i).type === 'I') {
         primaryKey = i;
-        console.log(`Primary key is ${primaryKey} and its value is ${gs1Array.GS1[primaryKey]}`);
+        console.log(
+          `Primary key is ${primaryKey} and its value is ${gs1Array.GS1[primaryKey]}`
+        );
         dlOrderedAIlist.push(getAIElement(i, gs1dlt, gs1Array.GS1, dateAIs));
         done.push(i);
       }
     }
     if (gs1dlt.aiQualifiers[primaryKey] !== undefined) {
-      gs1dlt.aiQualifiers[primaryKey].forEach(function(i) {
+      gs1dlt.aiQualifiers[primaryKey].forEach(function (i) {
         if (gs1Array.GS1[i] !== undefined) {
           dlOrderedAIlist.push(getAIElement(i, gs1dlt, gs1Array.GS1, dateAIs));
           done.push(i);
@@ -117,7 +146,8 @@
         done.push(i);
       }
     }
-    for (i in gs1Array.other) { // These are the non-GS1 elements that can occur in a DL URI. We don't know the labels
+    for (i in gs1Array.other) {
+      // These are the non-GS1 elements that can occur in a DL URI. We don't know the labels
       if (!dlOrderedAIlist.includes(i)) {
         let temp = {};
         temp['ai'] = i;
@@ -129,22 +159,23 @@
     let returnObject = sortElementString(gs1Array.GS1);
     returnObject['ol'] = dlOrderedAIlist;
     returnObject['dl'] = gs1DigitalLinkURI;
-    returnObject['licensingMO'] = await getLicensingMO(primaryKey, gs1Array.GS1[primaryKey]);
+    returnObject['licensingMO'] = await getLicensingMO(
+      primaryKey,
+      gs1Array.GS1[primaryKey]
+    );
     console.log(returnObject);
     return returnObject;
   }
 }
 
-
 function getAIElement(e, gs1dlt, values, dateAIs) {
   ro = {};
   ro['ai'] = e;
-  ro['label'] = gs1dlt.aitable.find(x => x.ai === e).label;
+  ro['label'] = gs1dlt.aitable.find((x) => x.ai === e).label;
   ro['value'] = dateAIs.includes(e) ? gs1ToISO(values[e]) : values[e];
   return ro;
 }
 
-
 function sortElementString(a) {
   // This creates two GS1 element string versions of the given array, one with brackets, one with FNC1
   // Order is:
@@ -155,47 +186,55 @@
   let gs1dlt = new GS1DigitalLinkToolkit();
   let sortedBrackets = '';
   let sortedFNC1 = '';
-//  const FNC1 = String.fromCharCode(29);
+  //  const FNC1 = String.fromCharCode(29);
   const FNC1 = gs1dlt.groupSeparator;
-  for (i in a) {    // Look for the primary key
-    if (gs1dlt.aitable.find(x => x.ai == i).type == 'I') {
+  for (i in a) {
+    // Look for the primary key
+    if (gs1dlt.aitable.find((x) => x.ai == i).type == 'I') {
       sortedBrackets = '(' + i + ')' + a[i];
       sortedFNC1 = FNC1 + i + a[i];
     }
   }
-  for (i in a) {    // Look for fixed length AIs
-    if ((sortedBrackets.indexOf('('+ i + ')') == -1) && (gs1dlt.aitable.find(x => x.ai == i).fixedLength == true)) {
+  for (i in a) {
+    // Look for fixed length AIs
+    if (
+      sortedBrackets.indexOf('(' + i + ')') == -1 &&
+      gs1dlt.aitable.find((x) => x.ai == i).fixedLength == true
+    ) {
       sortedBrackets += '(' + i + ')' + a[i];
       sortedFNC1 += i + a[i];
     }
   }
-  for (i in a) {    // Everything else
-    if (sortedBrackets.indexOf('('+ i + ')') == -1) {
+  for (i in a) {
+    // Everything else
+    if (sortedBrackets.indexOf('(' + i + ')') == -1) {
       sortedBrackets += '(' + i + ')' + a[i];
       sortedFNC1 += i + a[i] + FNC1;
     }
   }
-  if (sortedFNC1.lastIndexOf(FNC1) == sortedFNC1.length -1) { sortedFNC1 = sortedFNC1.substring(0, sortedFNC1.length -1)}
+  if (sortedFNC1.lastIndexOf(FNC1) == sortedFNC1.length - 1) {
+    sortedFNC1 = sortedFNC1.substring(0, sortedFNC1.length - 1);
+  }
   console.log(sortedBrackets);
   console.log(sortedFNC1);
-  return {'AIbrackets' : sortedBrackets, 'AIfnc1' : sortedFNC1}
+  return { AIbrackets: sortedBrackets, AIfnc1: sortedFNC1 };
 }
 function gs1ToISO(gs1Date) {
-  let rv="";
-  let regexDate= new RegExp("^\\d{6}$");
+  let rv = '';
+  let regexDate = new RegExp('^\\d{6}$');
   if (gs1Date !== undefined && regexDate.test(gs1Date)) {
     let doubleDigits = gs1Date.split(/(\d{2})/);
-    let year=parseInt(doubleDigits[1]);
-    let currentYear=new Date().getFullYear().toString();
-    let currentLastYY=parseInt(currentYear.substr(-2));
-    let currentFirstYY=parseInt(currentYear.substr(0,2));
-    let diff=year-currentLastYY;
-    let fullyear=currentFirstYY.toString()+year.toString();
-    if (diff >=51 && diff <= 99) {
-      fullyear=(currentFirstYY-1).toString()+year.toString();
+    let year = parseInt(doubleDigits[1]);
+    let currentYear = new Date().getFullYear().toString();
+    let currentLastYY = parseInt(currentYear.substr(-2));
+    let currentFirstYY = parseInt(currentYear.substr(0, 2));
+    let diff = year - currentLastYY;
+    let fullyear = currentFirstYY.toString() + year.toString();
+    if (diff >= 51 && diff <= 99) {
+      fullyear = (currentFirstYY - 1).toString() + year.toString();
     }
     if (diff >= -99 && diff <= -50) {
-      fullyear=(currentFirstYY+1).toString()+year.toString();
+      fullyear = (currentFirstYY + 1).toString() + year.toString();
     }
     if (fullyear !== undefined) {
       rv = fullyear + '-' + doubleDigits[3];
@@ -211,51 +250,52 @@
 // So this little function just %-encodes the ones we actually need to. There's probably a fancy way of doing this with
 // a single regex but, well... I hope you'll forgive the verbosity.
 function escapeReservedCharacters(str) {
-  str = str.replace("#", "%23");
-  str = str.replace("/", "%2F");
-  str = str.replace("%", "%25");
-  str = str.replace("&", "%26");
-  str = str.replace("+", "%2B");
-  str = str.replace(",", "%2C");
-  str = str.replace("!", "%21");
-  str = str.replace("(", "%28");
-  str = str.replace(")", "%29");
-  str = str.replace("*", "%2A");
-  str = str.replace("'", "%27");
-  str = str.replace(":", "%3A");
-  str = str.replace(";", "%3B");
-  str = str.replace("<", "%3C");
-  str = str.replace("=", "%3D");
-  str = str.replace("<", "%3E");
-  str = str.replace("?", "%3F");
+  str = str.replace('#', '%23');
+  str = str.replace('/', '%2F');
+  str = str.replace('%', '%25');
+  str = str.replace('&', '%26');
+  str = str.replace('+', '%2B');
+  str = str.replace(',', '%2C');
+  str = str.replace('!', '%21');
+  str = str.replace('(', '%28');
+  str = str.replace(')', '%29');
+  str = str.replace('*', '%2A');
+  str = str.replace("'", '%27');
+  str = str.replace(':', '%3A');
+  str = str.replace(';', '%3B');
+  str = str.replace('<', '%3C');
+  str = str.replace('=', '%3D');
+  str = str.replace('<', '%3E');
+  str = str.replace('?', '%3F');
   return str;
 }
 
 const getLicensingMO = async (primaryKey, pkValue) => {
   let mo;
   /* We need the list of MO prefixes which comes from a separate file */
-  await fetch("https://gs1.github.io/interpretGS1scan/MOprefixStrings.json")
-    .then(response => response.json())
-    .then(moPrefixes => {
-      const offset = primaryKey === '01' ? 1:0;
-      mo = moPrefixes.find(x => x.prefix === pkValue.substring(offset,x.prefix.length + offset)).mo;
+  await fetch('https://gs1.github.io/interpretGS1scan/MOprefixStrings.json')
+    .then((response) => response.json())
+    .then((moPrefixes) => {
+      const offset = primaryKey === '01' ? 1 : 0;
+      mo = moPrefixes.find(
+        (x) => x.prefix === pkValue.substring(offset, x.prefix.length + offset)
+      ).mo;
     });
   return mo;
-}
+};
 
 const getCoO = async (numeric) => {
   let country;
   /* We need the list of numeric country codes which comes from a separate file */
   /* Original source is https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes */
-  await fetch("https://gs1.github.io/interpretGS1scan/slim-2.json")
-    .then(response => response.json())
-    .then(isoCodes => {
-      country = isoCodes.find(x => x['country-code'] === numeric).name;
+  await fetch('https://gs1.github.io/interpretGS1scan/slim-2.json')
+    .then((response) => response.json())
+    .then((isoCodes) => {
+      country = isoCodes.find((x) => x['country-code'] === numeric).name;
     });
   return `${numeric} (${country})`;
-}
+};
 
-
 async function displayInterpretation(scan, outputNode) {
   let scanObj = await interpretScan(scan);
   outputNode.innerHTML = '';
@@ -267,8 +307,8 @@
     let p = document.createElement('p');
     p.classList.add('error');
     p.appendChild(document.createTextNode(scanObj.errmsg));
-    outputNode.appendChild(p);   
-  } else {    
+    outputNode.appendChild(p);
+  } else {
     let label = document.createElement('label');
     label.classList.add('sectionHeader');
     label.htmlFor = 'identifiers';
@@ -276,18 +316,19 @@
     outputNode.appendChild(label);
     let div = document.createElement('div');
     div.id = 'identifiers';
-    for (i in scanObj.ol) { // scanObj.ol is the ordered list we want to go through
+    for (i in scanObj.ol) {
+      // scanObj.ol is the ordered list we want to go through
       let p = document.createElement('p');
       p.id = '_' + scanObj.ol[i].ai;
       p.classList.add('aiDisplay');
       let span = document.createElement('span');
       span.classList.add('ai');
-      let ai = (scanObj.ol[i].ai == undefined) ? '' : scanObj.ol[i].ai;
+      let ai = scanObj.ol[i].ai == undefined ? '' : scanObj.ol[i].ai;
       span.appendChild(document.createTextNode(ai));
       p.appendChild(span);
       span = document.createElement('span');
       span.classList.add('aiLabel');
-      label = (scanObj.ol[i].label == undefined) ? '' : scanObj.ol[i].label;
+      label = scanObj.ol[i].label == undefined ? '' : scanObj.ol[i].label;
       span.appendChild(document.createTextNode(label));
       p.appendChild(span);
       span = document.createElement('span');
@@ -296,8 +337,13 @@
       span.classList.add('aiValue');
       let v = '';
       if (scanObj.ol[i].value !== undefined) {
-        console.log(`scanObj.ol[i].ai is ${scanObj.ol[i].ai} and the value is ${scanObj.ol[i].value}`)
-        v = (scanObj.ol[i].ai === "422") ? await getCoO(scanObj.ol[i].value) : scanObj.ol[i].value;
+        console.log(
+          `scanObj.ol[i].ai is ${scanObj.ol[i].ai} and the value is ${scanObj.ol[i].value}`
+        );
+        v =
+          scanObj.ol[i].ai === '422'
+            ? await getCoO(scanObj.ol[i].value)
+            : scanObj.ol[i].value;
       }
       span.appendChild(document.createTextNode(v));
       p.appendChild(span);
@@ -317,7 +363,9 @@
     let p = document.createElement('p');
     label = document.createElement('label');
     label.htmlFor = 'aiBrackets';
-    label.appendChild(document.createTextNode('Human-readable GS1 Element String syntax'));
+    label.appendChild(
+      document.createTextNode('Human-readable GS1 Element String syntax')
+    );
     let span = document.createElement('span');
     span.classList.add('syntax');
     span.id = 'aiBrackets';
@@ -329,7 +377,9 @@
     p = document.createElement('p');
     label = document.createElement('label');
     label.htmlFor = 'aiFNC1';
-    label.appendChild(document.createTextNode('Native GS1 Element String syntax'));
+    label.appendChild(
+      document.createTextNode('Native GS1 Element String syntax')
+    );
     span = document.createElement('span');
     span.classList.add('syntax');
     span.id = 'aiFNC1';
@@ -358,7 +408,9 @@
     label = document.createElement('label');
     label.htmlFor = 'licensingMO';
     label.classList.add('sectionHeader');
-    label.appendChild(document.createTextNode('Licensing GS1 Member Organisation'));
+    label.appendChild(
+      document.createTextNode('Licensing GS1 Member Organisation')
+    );
     outputNode.appendChild(label);
     div = document.createElement('div');
     div.id = 'licensingMO';
@@ -367,7 +419,5 @@
     div.appendChild(p);
 
     outputNode.appendChild(div);
-
   }
 }
-
