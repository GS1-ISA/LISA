# Technical Design Document: RooCode Indexing System Quota Management

### 1. Introduction

This document outlines a detailed plan for implementing robust quota management strategies within the RooCode indexing system. The primary goals are to optimize resource utilization, control operational costs (especially for embedding model API calls), and ensure the stability and efficiency of the indexing pipeline.

### 2. Current Indexing Architecture Overview

The RooCode indexing system leverages an ELTVRE (Extract, Load, Transform, Validate, Refine, Enrich) pipeline to process project data. Key components include:
*   **File Traversal & Content Extraction:** Reads various file types, including code and documentation.
*   **Code Parsing & AST Analysis:** Extracts structured code definitions.
*   **Chunking & Embedding:** Divides content into manageable chunks and generates vector embeddings using an external embedding model (e.g., `text-embedding-004`).
*   **Storage:** Utilizes a Vector Database (e.g., AlloyDB AI pgvector / Vertex AI Vector Search) for embeddings and a Knowledge Graph for structured metadata and relationships.
*   **Update Strategy:** Combines real-time monitoring for critical changes and scheduled full re-indexing.

### 3. Quota Management Strategies

#### 3.1. Batching

**Objective:** Reduce the number of API calls to embedding models and improve throughput by processing multiple chunks in a single request.

**Implementation Details:**
*   **Batching Layer:** Introduce a batching layer within the "Chunking & Embedding" component of the ELTVRE pipeline. This layer will collect document chunks until a predefined batch size or time limit is reached.
*   **Configuration:**
    *   `EMBEDDING_BATCH_SIZE`: Configurable parameter (e.g., 100-500 chunks per batch, depending on model limits and average chunk size).
    *   `EMBEDDING_BATCH_TIMEOUT_MS`: Configurable time limit (e.g., 500ms - 2000ms) to prevent indefinite waiting for a full batch.
*   **Process:**
    1.  Chunks are generated by the `text_chunker.py` and `file_extractor.py` modules.
    2.  Instead of sending each chunk individually, they are added to a queue managed by the batching layer.
    3.  When the queue reaches `EMBEDDING_BATCH_SIZE` or `EMBEDDING_BATCH_TIMEOUT_MS` expires, the accumulated chunks are sent as a single request to the embedding model API.
    4.  The embedding results are then mapped back to their respective chunks and passed down the pipeline for storage.
*   **Error Handling:** Implement robust error handling for batch requests (e.g., retry mechanisms for transient errors, splitting large batches into smaller ones if specific chunks cause issues).

**Estimated Effort:** Medium (2-3 days) - Requires modifying the embedding integration, implementing a queue, and configuring batch parameters.

#### 3.2. Rate Limiting

**Objective:** Prevent exceeding API rate limits imposed by embedding model providers and ensure fair usage across different indexing tasks.

**Implementation Details:**
*   **Token Bucket Algorithm:** Implement a token bucket algorithm for rate limiting API calls.
    *   **Bucket Capacity:** Represents the maximum burst of requests allowed (e.g., 1000 tokens).
    *   **Refill Rate:** Defines how many tokens are added to the bucket per unit of time (e.g., 100 tokens/second).
*   **Centralized Rate Limiter:** A dedicated rate limiting service or module will manage access to the embedding model API. All embedding requests (batched or individual) must pass through this limiter.
*   **Configuration:**
    *   `EMBEDDING_API_RATE_LIMIT_TOKENS_PER_SECOND`: Maximum tokens per second allowed.
    *   `EMBEDDING_API_BURST_CAPACITY_TOKENS`: Maximum burst capacity.
*   **Integration:** The batching layer will request tokens from the rate limiter before sending a batch. If insufficient tokens are available, the request will be delayed until tokens are replenished.
*   **Monitoring:** Integrate rate limiter metrics into the cost monitoring system.

**Estimated Effort:** Medium (2-3 days) - Requires implementing the token bucket logic and integrating it with the batching layer.

#### 3.3. Incremental Indexing

**Objective:** Minimize re-indexing efforts and associated costs by only processing changed or new files/chunks.

**Implementation Details:**
*   **Change Detection:**
    *   **File Hashes:** Store a hash (e.g., MD5 or SHA256) of each indexed file's content in the Metadata Store. During scheduled re-indexing, compare the current file's hash with the stored hash. If they differ, the file has changed and needs re-indexing.
    *   **Timestamps:** Utilize file modification timestamps (`mtime`). If a file's `mtime` is newer than its last indexed timestamp, it's a candidate for re-indexing. This is a lighter check than hashing.
*   **Granular Updates:**
    *   **Chunk-level Hashing:** For large files, consider hashing individual chunks. If only a few chunks within a large file change, only those specific chunks need to be re-embedded and updated in the vector database.
    *   **Deletion Handling:** When a file is deleted, its corresponding entries in the Vector Database, Knowledge Graph, and Metadata Store must be removed.
*   **Optimized Scheduled Re-indexing:** The scheduled re-indexing job will first perform change detection and then only process the identified changed/new files or chunks.
*   **Real-time Monitoring Integration:** The existing real-time file system event monitoring will directly trigger incremental updates for affected files.

**Estimated Effort:** High (5-7 days) - Requires significant logic for change detection, granular updates, and deletion handling across multiple storage solutions.

### 4. Integration Points for Cost Monitoring and Alerting with GCP Services

**Objective:** Gain visibility into indexing costs and receive timely alerts for budget overruns or unusual spending patterns.

**Implementation Details:**
*   **GCP Billing Export:** Enable GCP Billing Export to BigQuery. This provides detailed cost data that can be analyzed.
*   **Custom Metrics (Cloud Monitoring):**
    *   **Embedding API Calls:** Publish custom metrics for the number of embedding API calls made (batched and individual).
    *   **Tokens Processed:** Publish metrics for the total number of tokens sent to embedding models.
    *   **Indexing Throughput:** Metrics for files/chunks processed per unit of time.
*   **Cloud Logging:** Log all embedding API requests and responses, including cost-related metadata (e.g., token count, model used).
*   **Cloud Functions/Cloud Run for Cost Analysis:**
    *   Develop a Cloud Function or Cloud Run service that periodically queries BigQuery billing data and Cloud Monitoring metrics.
    *   Analyze trends, identify cost anomalies, and calculate projected spending.
*   **Cloud Monitoring Alerting:**
    *   **Budget Alerts:** Configure budget alerts in GCP Billing to notify stakeholders when spending approaches predefined thresholds.
    *   **Custom Metric Alerts:** Set up alerts on custom metrics (e.g., "embedding API calls exceeding X per hour," "cost per indexed item increasing").
    *   **Anomaly Detection:** Utilize Cloud Monitoring's anomaly detection capabilities to identify sudden spikes in cost or usage.
*   **Notification Channels:** Integrate alerts with preferred notification channels (e.g., email, Slack, PagerDuty).

**Estimated Effort:** Medium (4-5 days) - Requires setting up GCP services, developing custom metric publishing, and implementing cost analysis logic.

### 5. Consideration of Different Embedding Model Tiers or Alternatives for Cost Optimization

**Objective:** Reduce embedding costs by strategically using different models based on content importance or by exploring open-source alternatives.

**Implementation Details:**
*   **Tiered Embedding Strategy:**
    *   **High-Fidelity Tier (e.g., `text-embedding-004`):** Used for core code, critical documentation (`isa/docs`, `src/ai/flows`), and high-value content where semantic accuracy is paramount for search and RAG.
    *   **Lower-Cost Tier (e.g., smaller Google models, open-source models like `all-MiniLM-L6-v2`):** Used for less critical content like log files, temporary files, or less frequently accessed documentation, where a slightly lower embedding quality is acceptable for significant cost savings.
*   **Model Selection Logic:**
    *   Implement a `ModelSelectionService` that determines which embedding model to use based on file path, file type, content importance (e.g., defined in a configuration file or inferred).
    *   This service will be integrated into the "Embedding Generation" step of the ELTVRE pipeline.
*   **Open-Source Embedding Models (Self-Hosted):**
    *   **Exploration:** Research and evaluate suitable open-source embedding models (e.g., from Hugging Face Transformers, Sentence Transformers).
    *   **Deployment:** If a suitable model is found, deploy it on a cost-effective GCP compute instance (e.g., GKE, Cloud Run, or a dedicated VM with GPU if needed).
    *   **Integration:** The `ModelSelectionService` would then route requests to this self-hosted endpoint for the lower-cost tier.
    *   **Trade-offs:** Consider the operational overhead, maintenance, and potential performance implications of self-hosting vs. managed services.

**Estimated Effort:** High (7-10 days) - Requires research, evaluation, potential deployment of new services, and integration with the indexing pipeline.

### 6. Proposal for a Local Embedding Cache

**Objective:** Drastically reduce embedding API calls and latency by caching previously generated embeddings locally.

**Implementation Details:**
*   **Cache Location:**
    *   **In-memory Cache (for active indexing session):** A simple in-memory cache (e.g., using a dictionary or LRU cache) for embeddings generated within a single indexing run. This is fast but ephemeral.
    *   **Persistent Disk Cache:** For longer-term persistence and across indexing runs, use a local disk-based cache.
        *   **SQLite Database:** A lightweight SQLite database (`isa/cache/embedding_cache.db`) can store `(content_hash, embedding_vector)` pairs.
        *   **Key-Value Store:** Alternatively, a simple file-based key-value store where the key is the content hash and the value is the serialized embedding vector.
*   **Cache Key:** The cache key will be a hash of the chunk's content (e.g., SHA256). This ensures that identical content always maps to the same embedding.
*   **Cache Lookup:** Before sending a chunk to the embedding model, the system will first check the local cache using the content hash.
*   **Cache Invalidation/Update:**
    *   **Content Change:** If a chunk's content changes (detected by incremental indexing), its entry in the cache must be updated or invalidated.
    *   **Model Change:** If the embedding model used for a chunk changes (e.g., due to tiered strategy), the cached embedding for that chunk is no longer valid and must be re-generated.
    *   **Cache Eviction Policy:** Implement an LRU (Least Recently Used) or LFU (Least Frequently Used) policy for the persistent cache to manage its size.
*   **Integration:** The cache will sit between the "Chunking" and "Embedding Generation" steps.

```mermaid
graph TD
    A[Chunked Content] --> B{Check Local Cache (Content Hash)};
    B -- Cache Hit --> C[Retrieve Embedding from Cache];
    B -- Cache Miss --> D[Send to Embedding Model (Batched & Rate Limited)];
    D --> E[Receive Embedding];
    E --> F[Store Embedding in Cache];
    C & F --> G[Indexed Content (with Embedding)];
```

**Estimated Effort:** High (5-7 days) - Requires implementing cache logic, persistence mechanism, and careful integration with the existing pipeline and incremental indexing.

### 7. Overall Architecture Diagram with Quota Management

```mermaid
graph TD
    subgraph ELTVRE Pipeline
        A[Project Directory] --> B(File Traversal & Filtering);
        B --> C{File Type?};
        C -- Text/Code --> D[Content Extraction & Parsing];
        C -- Binary/PDF --> E[Specialized Parsers];
        D --> F[Code Definition Extraction];
        E --> G[Text Content];
        F --> H[Structured Code Data];
        G & H --> I[Chunking];
        I --> J{Local Embedding Cache};
        J -- Cache Hit --> K[Retrieve Embedding from Cache];
        J -- Cache Miss --> L[Model Selection Service];
        L --> M[Batching Layer];
        M --> N[Rate Limiter];
        N --> O[External Embedding Model API];
        O --> P[Receive Embedding];
        P --> Q[Store Embedding in Cache];
        K & Q --> R[Embeddings];
        R --> S[Vector Database];
        F --> T[Relationship Extraction];
        T --> U[Knowledge Graph];
        B --> V[Metadata Extraction];
        V --> W[Metadata Store];
    end

    subgraph Quota Management & Monitoring
        O -- Usage Data --> X[GCP Cloud Logging];
        P -- Token Count --> Y[GCP Cloud Monitoring (Custom Metrics)];
        Y --> Z[Cloud Monitoring Alerts];
        X & Y --> AA[BigQuery Billing Export];
        AA --> BB[Cloud Functions/Run (Cost Analysis)];
        BB --> Z;
    end

    subgraph Update Strategy
        CC[Real-time File System Monitoring] --> I;
        DD[Scheduled Re-indexing Job] --> B;
        EE[Git Hooks] --> B;
    end

    S & U & W --> FF[Indexed Project Knowledge];
    FF --> GG[Roo Modes (Search, RAG, Analysis, Debugging, Architect)];
```

### 8. Estimated Overall Effort

*   **Phase 1: Core Quota Mechanisms (Batching, Rate Limiting)**: ~4-6 days
*   **Phase 2: Incremental Indexing & Cache (Local Cache, Change Detection)**: ~10-14 days
*   **Phase 3: Cost Optimization & Monitoring (Tiered Models, GCP Monitoring)**: ~11-15 days
*   **Documentation & Testing (Ongoing)**: ~5 days (spread across phases)

**Total Estimated Effort:** Approximately 30-40 person-days.

### 9. Next Steps

Upon approval of this design document, the next step will be to create a detailed implementation roadmap, breaking down each phase into actionable tasks and assigning priorities.